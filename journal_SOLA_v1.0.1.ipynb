{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.1\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.get(\"https://www.jstage.jst.go.jp/browse/sola/list/-char/en\")\n",
    "    \n",
    "    # get all of the issue\n",
    "    def issues():\n",
    "        issues = []\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for a in soup.find_all('a', {'class': 'bluelink-style'}):\n",
    "            # if it is this journal\n",
    "            if re.search(\"Volume\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                issues.append(a['href'])        \n",
    "        \n",
    "        return  issues    \n",
    "    allissue = issues()\n",
    "    allissue = list(set(allissue))\n",
    "    \n",
    "    def articles():\n",
    "        articles = []\n",
    "        nexpage = False\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for a in soup.find_all('a'):\n",
    "            # if it is this journal\n",
    "            if re.search(\"^https://doi.org\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                articles.append(a['href'])\n",
    "        # check there is new issue\n",
    "        for a in driver.find_elements_by_name('a'):\n",
    "            if '>' == a.getText():\n",
    "                nexpage = True\n",
    "                # set to nexpage\n",
    "                a.click()\n",
    "                sleep(3)        \n",
    "        return  articles, nexpage\n",
    "    \n",
    "    # iterate over isses and get article links\n",
    "    articellinks = []\n",
    "    for issue in allissue:\n",
    "        driver.get(issue)\n",
    "        sleep(2)\n",
    "        print(driver.current_url, end='\\r') \n",
    "        # get the issue article\n",
    "        nextpage = True\n",
    "        allissue = []\n",
    "        while nextpage:\n",
    "            thispagearticles, nextpage = articles()\n",
    "            articellinks = articellinks + thispagearticles\n",
    "            print(driver.current_url, end='\\r')        \n",
    "    articellinks = list(set(articellinks))\n",
    "    \n",
    "    # filter out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articellinks = list( set(articellinks).difference(set(oldarticles) ))\n",
    "    print(len(articellinks), \"new article found!\")\n",
    "    \n",
    "    # iterate over article \n",
    "    urls = [ None for _ in range(len(articellinks))]\n",
    "    titles = [ None for _ in range(len(articellinks))]\n",
    "    abstracts = [ None for _ in range(len(articellinks))]\n",
    "    writers = [ None for _ in range(len(articellinks))]\n",
    "    dates = [ None for _ in range(len(articellinks))]\n",
    "    dois = [  None for _ in range(len(articellinks)) ]\n",
    "    keywords = [  None for _ in range(len(articellinks)) ]\n",
    "    for idx in range(len(articellinks)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(articellinks),2))+\"%\" , end='\\r')\n",
    "        url =  articellinks[idx]\n",
    "        # load page\n",
    "        driver.get(articellinks[idx])\n",
    "        sleep(3)        \n",
    "        urls[idx] = driver.current_url\n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # process metadata\n",
    "        keyword = []\n",
    "        writter = []\n",
    "        notpaper = False\n",
    "        for m in soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == 'title':\n",
    "                    titles[idx] = m['content']\n",
    "                if m['name'] == \"citation_keywords\":\n",
    "                    keyword.append( m['content'] )\n",
    "                if m['name'] == \"citation_online_date\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'citation_author':\n",
    "                    writter.append(m['content'])\n",
    "                if re.search('citation_doi', m['name']):\n",
    "                    dois[idx] = m['content']                    \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        if len(keyword) > 0:\n",
    "            keywords[idx] =  \"#\".join(keyword)\n",
    "            \n",
    "        # get abstract\n",
    "        abstract = \"\"\n",
    "        if notpaper:\n",
    "            continue\n",
    "        else:\n",
    "            for div in soup.find_all(id='article-overiew-abstract-wrap'):\n",
    "                for c in div.find_all(['p', 'script']):\n",
    "                    if c.name == \"p\":                    \n",
    "                        for tag in  c.contents:\n",
    "                            if re.search(\"<span\", str(tag)):\n",
    "                                continue\n",
    "                            else:\n",
    "                                match = re.match(\"<script.+>\", str(tag))\n",
    "                                if match is not None:\n",
    "                                    abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                                else:\n",
    "                                    abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "                # if still not abstract it is possible it is not in p\n",
    "                if len(abstract) == 0:\n",
    "                    for tag in div.contents:\n",
    "                        if re.search(\"<span\", str(tag)) or re.search(\"<h3\", str(tag)) :\n",
    "                            continue\n",
    "                        else:\n",
    "                            match = re.match(\"<script.+>\", str(tag))\n",
    "                            if match is not None:\n",
    "                                abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                            else:\n",
    "                                abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "            abstracts[idx] = abstract       \n",
    "        \n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': urls, \n",
    "                       'journal_title': \"SOLA\", \n",
    "                       'journal_eissn': \"1349-6476\",\n",
    "                       'journal_pissn': '',\n",
    "                       'category': \"Meteorology. Climatology\",                                              \n",
    "                      })\n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # drop where is no abstract\n",
    "    df = df[ df['abstract'].notna()]\n",
    "    # drop where title is Editorial\n",
    "    df = df[df['title'] != 'Editorial']\n",
    "    # drop where title \"The SOLA Award in *\"\"\n",
    "    df = df[df.title.str.contains('^(?!The SOLA Award in)', regex= True, na=False)]\n",
    "    \n",
    "    \n",
    "    # check there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762 new article found!.go.jp/browse/sola/6/0/_contents/-char/enen\n",
      "                                                 url journal_title  \\\n",
      "0  https://www.jstage.jst.go.jp/article/sola/10/0...          SOLA   \n",
      "1  https://www.jstage.jst.go.jp/article/sola/7/0/...          SOLA   \n",
      "2  https://www.jstage.jst.go.jp/article/sola/17/0...          SOLA   \n",
      "3  https://www.jstage.jst.go.jp/article/sola/8/0/...          SOLA   \n",
      "4  https://www.jstage.jst.go.jp/article/sola/5/0/...          SOLA   \n",
      "\n",
      "  journal_eissn journal_pissn                  category  \\\n",
      "0     1349-6476                Meteorology. Climatology   \n",
      "1     1349-6476                Meteorology. Climatology   \n",
      "2     1349-6476                Meteorology. Climatology   \n",
      "3     1349-6476                Meteorology. Climatology   \n",
      "4     1349-6476                Meteorology. Climatology   \n",
      "\n",
      "                                               title                    doi  \\\n",
      "0  Cooling by the Melting of Snowfall on the Toya...  10.2151/sola.2014-031   \n",
      "1  Impact of Pollutant Transport on the Air Quali...  10.2151/sola.2011-022   \n",
      "2  Regional Characteristics of Future Changes in ...  10.2151/sola.2021-001   \n",
      "3  An Empirical Study of the Response of the Sout...  10.2151/sola.2012-017   \n",
      "4  A Simulated Preconditioning of Typhoon Genesis...  10.2151/sola.2009-017   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  The peaks of the appearance frequency of the s...   \n",
      "1  Pollutant transport has a significant impact o...   \n",
      "2  We investigate regional characteristics of fut...   \n",
      "3  A self-consistent mechanistic model is conside...   \n",
      "4  A preconditioning of a typhoon genesis over th...   \n",
      "\n",
      "                                              writer publishdate keyword  \n",
      "0  Takao Yoshikane#Hiroaki Hatsushika#Hiroaki Kaw...  2014-10-04    None  \n",
      "1  Pingzhong Yan#Zifa Wang#Xiquan Wang#Qingyan Fu...  2011-06-16    None  \n",
      "2  Hiroaki Kawase#Akihiko Murata#Ken Yamada#Tosiy...  2021-01-27    None  \n",
      "3        Rosbintarti Kartika Lestari#Toshiki Iwasaki  2012-06-02    None  \n",
      "4  Kazuyoshi Oouchi#Akira T. Noda#Masaki Satoh#Hi...  2009-04-23    None  \n"
     ]
    }
   ],
   "source": [
    "filename = 'journal_SOLA_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "\n",
    "# save data\n",
    "df.to_pickle(os.path.join(datadir, filename))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peaks of the appearance frequency of the surface air temperature during precipitation are clearly observed near the melting point of water on the Toyama Plain during the winter monsoon. The peaks could be explained by the hypothesis that the melting of snowfall is the primary cause of the cooling on the Toyama Plain. To verify this hypothesis, we investigated the relation of temperature between the inland and the coast using observed data in January from 1990 to 2009 and applied a simple estimation method of the cooling due to the melting of snowfall. The temperature on the Toyama Plain tends to remain around the melting point when the surface air temperature on the coast is higher than 273.15 K and lower than 277.15 K, which almost corresponds to the changeover from snowfall to rainfall. The relation is unclear when hardly any precipitation is observed. The simply estimated cooling by the melting of snowfall using the observed precipitation can also represents the cooling on the Toyama Plain. Accordingly, the local climatic temperature could be greatly influenced by advection of the air mass cooled by the melting of snowfall until the air mass reaches the Toyama Plain during the winter monsoon.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url              https://www.jstage.jst.go.jp/article/sola/10/0...\n",
      "journal_title                                                 SOLA\n",
      "journal_eissn                                            1349-6476\n",
      "journal_pissn                                                     \n",
      "category                                  Meteorology. Climatology\n",
      "title            Cooling by the Melting of Snowfall on the Toya...\n",
      "doi                                          10.2151/sola.2014-031\n",
      "abstract         The peaks of the appearance frequency of the s...\n",
      "writer           Takao Yoshikane#Hiroaki Hatsushika#Hiroaki Kaw...\n",
      "publishdate                                             2014-10-04\n",
      "keyword                                                       None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
