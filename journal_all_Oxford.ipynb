{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The Article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.set_window_size(1920, 1080)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(journal, filename ):\n",
    "    start_url = \"https://academic.oup.com/journals/advanced-search\"\n",
    "    driver.get(start_url)\n",
    "    print(driver.current_url)\n",
    "    sleep(4)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    \n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [], 'journal_title': [],\n",
    "                                'title': [], 'writer': [], \n",
    "                                'doi': [], 'abstract': [],\n",
    "                                'keyword': [], 'publishdate': [],\n",
    "                                'language': [],\n",
    "                                'category': [],\n",
    "                                'journal_eissn': [], 'journal_pissn': []})\n",
    "    olddf.set_index('doi', inplace=True)\n",
    "    olddf['doi'] = []\n",
    "    try:\n",
    "        olddf = pd.read_pickle(filename)\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df =  pd.DataFrame({'url': [], 'journal_title': [],\n",
    "                                'title': [], 'writer': [], \n",
    "                                'doi': [], 'abstract': [],\n",
    "                                'keyword': [], 'publishdate': [],\n",
    "                                'language': [],\n",
    "                                'category': [],\n",
    "                                'journal_eissn': [], 'journal_pissn': []})\n",
    "    df.set_index('doi', inplace=True)\n",
    "    df['doi'] = []\n",
    "\n",
    "    # set journal\n",
    "    try:\n",
    "        selectpublisher = Select(driver.find_element_by_id(\"SelectedJournal\"))\n",
    "    except Exception as e:\n",
    "        print(\"Error in SelectedJournal: \", e)\n",
    "        return df, olddf\n",
    "    print(journal['title'].split(\":\")[0])\n",
    "    searchtitle = journal['title'].split(\":\")[0]\n",
    "    try:        \n",
    "        # patch for specific articles\n",
    "        if searchtitle == 'AoB Plants':\n",
    "            searchtitle = 'AoB PLANTS'\n",
    "        \n",
    "        selectpublisher.select_by_visible_text(searchtitle)                        \n",
    "    except Exception as e:\n",
    "        print(\"Error in journal selection:\", e)\n",
    "        return df, olddf\n",
    "    a = driver.find_element_by_id(\"btnAdvancedCitationFilter\")\n",
    "    if a is not None:\n",
    "        a.click()\n",
    "    sleep(4)\n",
    "    print(driver.current_url)\n",
    "\n",
    "    # just for Research articles\n",
    "    try:\n",
    "        driver.find_element_by_id(\"article-type-label0\").click()\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    def processresult(df):\n",
    "        sleep(np.random.uniform(3,60))        \n",
    "        \n",
    "    \n",
    "        # open all of the abstract\n",
    "        for selabstract in driver.find_elements_by_class_name('js-show-abstract'):\n",
    "            selabstract.click()\n",
    "            sleep(np.random.uniform(3,120))\n",
    "        \n",
    "        # BeautifulSoup\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get the articles \n",
    "        articles = soup.find_all('div', {'class': ['al-article-box']})\n",
    "        \n",
    "\n",
    "    \n",
    "        for article in articles:\n",
    "            title = article.find(\"h4\").get_text()\n",
    "            url = article.find(\"h4\").find(\"a\")['href']\n",
    "            print(\"url\", url)\n",
    "            date = article.find('div', {'class': ['al-pub-date']}).get_text().replace(\"Published: \", '')\n",
    "            date = datetime.datetime.strptime(date, \"%d %B %Y\").strftime('%Y-%m-%d')        \n",
    "            abstract = article.find('section', {'class': ['abstract']}).get_text()\n",
    "            abstract = re.sub(searchtitle+' URL: .+', '', abstract )\n",
    "            doi = \"\"\n",
    "            doidiv = article.find('div', {'class': ['al-citation-list']}).find_all('a')\n",
    "            for a in doidiv:\n",
    "                if re.search(\"doi.org\", a[\"href\"] ):\n",
    "                    doi = a[\"href\"].replace(\"https://doi.org/\", '')\n",
    "                    break\n",
    "            writers = []\n",
    "            writersdeiv =  article.find('div', {'class': ['al-authors-list']}).find_all( 'span', {'class': ['wi-fullname']} )\n",
    "            for w in writersdeiv:\n",
    "                writers.append(w.get_text())\n",
    "            writers = \"#\".join(writers)\n",
    "        \n",
    "            # add data to the df\n",
    "            datadict = pd.Series({\n",
    "                        'journal_title': journal['title'],\n",
    "                        'url': url,\n",
    "                        'title': title,\n",
    "                        'writer': writers,\n",
    "                        'doi': doi,\n",
    "                        'abstract': abstract,\n",
    "                        'keyword': \"\",\n",
    "                        'publishdate': date,\n",
    "                        'language': \"en\",\n",
    "                        'journal_eissn': journal['eissn'],\n",
    "                        'journal_pissn': journal['pissn'],\n",
    "                        'category': journal['categories']\n",
    "                    }).rename(doi)\n",
    "            df = df.append(datadict) \n",
    "        df.to_pickle(filename)\n",
    "        print(df.tail())\n",
    "        return df\n",
    "    \n",
    "    if not re.search('JournalDisplayName', driver.current_url):\n",
    "        print(\"Error to set Journal:\", driver.current_url)\n",
    "        return df, olddf\n",
    "    \n",
    "    # firs result\n",
    "    df = processresult(df)        \n",
    "    \n",
    "    # iterate over the nexts\n",
    "    while True:\n",
    "        try:\n",
    "            sleep(np.random.uniform(3,60))\n",
    "            for nextbottom in driver.find_elements_by_css_selector(\"div.pagination a.al-nav-next\"):\n",
    "                nextbottom.click()\n",
    "                break\n",
    "            df = processresult(df)\n",
    "        except:\n",
    "            try:\n",
    "                for nextbottom in driver.find_elements_by_css_selector(\"div.al-pageNumbers a.al-nav-next\"):\n",
    "                    nextbottom.click()\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(\"Error on next:\", e)\n",
    "                break\n",
    "        \n",
    "    return df, olddf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ddfcb11f3e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjournals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doaj_journals.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjournals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjournals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'publisher'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Oxford University Press\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjournals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjournals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "journals = pd.read_csv(\"doaj_journals.csv\", sep=\"|\")\n",
    "journals = journals[journals['publisher'] == \"Oxford University Press\"]\n",
    "journals = journals.to_dict('records')\n",
    "print(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'journals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8bb44b504556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mjidx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjournals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/journal_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"&\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"and\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# search for articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0molddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgedjournalarticles\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'journals' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for jidx in range(len(journals)):\n",
    "    filename = filename = 'data/journal_'+ journals[jidx]['title'].replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\") +'_'+version+'.pandas'\n",
    "    \n",
    "    # search for articles\n",
    "    df, olddf = gedjournalarticles( journals[jidx], filename )\n",
    "    \n",
    "    # merg olddf and df\n",
    "    df = pd.concat([df,olddf], axis=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
