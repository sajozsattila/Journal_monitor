{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.get(\"https://scholarship.claremont.edu/jhm/all_issues.html\")\n",
    "    \n",
    "    # get all of the issue\n",
    "    def issues():\n",
    "        issues = []\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for h3 in soup.find_all('h3', {'class': 'issue'}):\n",
    "            for a in h3.find_all('a'):                \n",
    "                issues.append(a['href'])        \n",
    "        \n",
    "        return  issues    \n",
    "    allissue = issues()\n",
    "    allissue = list(set(allissue))\n",
    "    print(allissue)\n",
    "    \n",
    "    def articles():\n",
    "        articles = []\n",
    "        nexpage = False\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # We collect the texts under the Articles         \n",
    "        # get the 'Artilces' h2\n",
    "        for h2 in soup.find_all('h2'):\n",
    "            if h2.string != \"Articles\":\n",
    "                continue\n",
    "            for sib in h2.next_siblings:\n",
    "                if sib.name == 'div':\n",
    "                    for a in sib.find_all(\"a\"):\n",
    "                        if not re.search(\"cgi\", a['href']):\n",
    "                            articles.append(a['href'])\n",
    "                elif sib.name == 'h2':\n",
    "                    break\n",
    "        return  articles\n",
    "    \n",
    "    # iterate over isses and get article links\n",
    "    articellinks = []\n",
    "    for issue in allissue:\n",
    "        driver.get(issue)\n",
    "        sleep(2)\n",
    "        print(driver.current_url, end='\\r')         \n",
    "        thispagearticles = articles()\n",
    "        articellinks = articellinks + thispagearticles        \n",
    "    articellinks = list(set(articellinks))\n",
    "    print(articellinks)\n",
    "    \n",
    "    # filter out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articellinks = list( set(articellinks).difference(set(oldarticles) ))\n",
    "    print(len(articellinks), \"new article found!\")\n",
    "    \n",
    "    # iterate over article \n",
    "    urls = [ None for _ in range(len(articellinks))]\n",
    "    titles = [ None for _ in range(len(articellinks))]\n",
    "    abstracts = [ None for _ in range(len(articellinks))]\n",
    "    writers = [ None for _ in range(len(articellinks))]\n",
    "    dates = [ None for _ in range(len(articellinks))]\n",
    "    dois = [  None for _ in range(len(articellinks)) ]\n",
    "    keywords = [  None for _ in range(len(articellinks)) ]\n",
    "    for idx in range(len(articellinks)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(articellinks),2))+\"%\" , end='\\r')\n",
    "        url =  articellinks[idx]\n",
    "        # load page\n",
    "        driver.get(articellinks[idx])\n",
    "        sleep(3)        \n",
    "        urls[idx] = driver.current_url\n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # process metadata\n",
    "        keyword = []\n",
    "        writter = []\n",
    "        abstract = \"\"\n",
    "        for m in soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == \"keywords\":\n",
    "                    keyword = m['content'].split(\",\")\n",
    "                if m['name'] == \"bepress_citation_online_date\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'author':\n",
    "                    writter.append(m['content'])\n",
    "                if m['name'] == 'bepress_citation_doi':\n",
    "                    dois[idx] = m['content']\n",
    "                if m['name'] == 'description':\n",
    "                    abstracts[idx] = abstract = m['content']\n",
    "            if m.has_attr(\"property\"):                    \n",
    "                if m['property'] == 'og:title':\n",
    "                    titles[idx] = m['content']\n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        if len(keyword) > 0:\n",
    "            keywords[idx] =  \"#\".join(keyword)\n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': urls, \n",
    "                       'journal_title': \"Journal of Humanistic Mathematics\", \n",
    "                       'journal_eissn': \"2159-8118\",\n",
    "                       'journal_pissn': '',\n",
    "                       'category': \"Mathematics\",                                              \n",
    "                      })\n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # drop where is no abstract\n",
    "    df = df[ df['abstract'].notna()]\n",
    "    # drop where title is Editorial\n",
    "    df = df[df['title'] != 'Editorial']    \n",
    "    \n",
    "    # check there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://scholarship.claremont.edu/jhm/vol7/iss2/', 'https://scholarship.claremont.edu/jhm/vol9/iss2/', 'https://scholarship.claremont.edu/jhm/vol10/iss1/', 'https://scholarship.claremont.edu/jhm/vol3/iss2/', 'https://scholarship.claremont.edu/jhm/vol5/iss1/', 'https://scholarship.claremont.edu/jhm/vol11/iss1/', 'https://scholarship.claremont.edu/jhm/vol6/iss2/', 'https://scholarship.claremont.edu/jhm/vol4/iss2/', 'https://scholarship.claremont.edu/jhm/vol2/iss2/', 'https://scholarship.claremont.edu/jhm/vol8/iss2/', 'https://scholarship.claremont.edu/jhm/vol6/iss1/', 'https://scholarship.claremont.edu/jhm/vol8/iss1/', 'https://scholarship.claremont.edu/jhm/vol1/iss1/', 'https://scholarship.claremont.edu/jhm/vol3/iss1/', 'https://scholarship.claremont.edu/jhm/vol9/iss1/', 'https://scholarship.claremont.edu/jhm/vol4/iss1/', 'https://scholarship.claremont.edu/jhm/vol5/iss2/', 'https://scholarship.claremont.edu/jhm/vol1/iss2/', 'https://scholarship.claremont.edu/jhm/vol2/iss1/', 'https://scholarship.claremont.edu/jhm/vol7/iss1/', 'https://scholarship.claremont.edu/jhm/vol10/iss2/']\n",
      "['https://scholarship.claremont.edu/jhm/vol2/iss1/3', 'https://scholarship.claremont.edu/jhm/vol10/iss2/5', 'https://scholarship.claremont.edu/jhm/vol8/iss2/4', 'https://scholarship.claremont.edu/jhm/vol6/iss1/6', 'https://scholarship.claremont.edu/jhm/vol10/iss2/8', 'https://scholarship.claremont.edu/jhm/vol7/iss1/4', 'https://scholarship.claremont.edu/jhm/vol9/iss1/3', 'https://scholarship.claremont.edu/jhm/vol6/iss1/8', 'https://scholarship.claremont.edu/jhm/vol5/iss2/4', 'https://scholarship.claremont.edu/jhm/vol6/iss2/7', 'https://scholarship.claremont.edu/jhm/vol10/iss1/4', 'https://scholarship.claremont.edu/jhm/vol4/iss2/4', 'https://scholarship.claremont.edu/jhm/vol5/iss1/6', 'https://scholarship.claremont.edu/jhm/vol6/iss2/3', 'https://scholarship.claremont.edu/jhm/vol6/iss2/6', 'https://scholarship.claremont.edu/jhm/vol8/iss1/8', 'https://scholarship.claremont.edu/jhm/vol7/iss1/6', 'https://scholarship.claremont.edu/jhm/vol10/iss1/5', 'https://scholarship.claremont.edu/jhm/vol6/iss1/10', 'https://scholarship.claremont.edu/jhm/vol10/iss1/6', 'https://scholarship.claremont.edu/jhm/vol7/iss1/3', 'https://scholarship.claremont.edu/jhm/vol10/iss1/7', 'https://scholarship.claremont.edu/jhm/vol6/iss1/4', 'https://scholarship.claremont.edu/jhm/vol5/iss2/6', 'https://scholarship.claremont.edu/jhm/vol11/iss1/5', 'https://scholarship.claremont.edu/jhm/vol8/iss1/4', 'https://scholarship.claremont.edu/jhm/vol7/iss2/3', 'https://scholarship.claremont.edu/jhm/vol1/iss1/3', 'https://scholarship.claremont.edu/jhm/vol10/iss2/7', 'https://scholarship.claremont.edu/jhm/vol8/iss2/6', 'https://scholarship.claremont.edu/jhm/vol10/iss1/11', 'https://scholarship.claremont.edu/jhm/vol8/iss1/6', 'https://scholarship.claremont.edu/jhm/vol8/iss2/5', 'https://scholarship.claremont.edu/jhm/vol1/iss1/4', 'https://scholarship.claremont.edu/jhm/vol9/iss2/6', 'https://scholarship.claremont.edu/jhm/vol7/iss1/5', 'https://scholarship.claremont.edu/jhm/vol4/iss2/3', 'https://scholarship.claremont.edu/jhm/vol2/iss1/4', 'https://scholarship.claremont.edu/jhm/vol11/iss1/3', 'https://scholarship.claremont.edu/jhm/vol3/iss1/3', 'https://scholarship.claremont.edu/jhm/vol9/iss2/5', 'https://scholarship.claremont.edu/jhm/vol9/iss1/7', 'https://scholarship.claremont.edu/jhm/vol10/iss1/10', 'https://scholarship.claremont.edu/jhm/vol6/iss1/5', 'https://scholarship.claremont.edu/jhm/vol10/iss2/9', 'https://scholarship.claremont.edu/jhm/vol6/iss1/11', 'https://scholarship.claremont.edu/jhm/vol7/iss1/8', 'https://scholarship.claremont.edu/jhm/vol10/iss2/10', 'https://scholarship.claremont.edu/jhm/vol6/iss2/5', 'https://scholarship.claremont.edu/jhm/vol9/iss1/6', 'https://scholarship.claremont.edu/jhm/vol5/iss2/5', 'https://scholarship.claremont.edu/jhm/vol6/iss1/7', 'https://scholarship.claremont.edu/jhm/vol8/iss1/3', 'https://scholarship.claremont.edu/jhm/vol11/iss1/7', 'https://scholarship.claremont.edu/jhm/vol6/iss2/4', 'https://scholarship.claremont.edu/jhm/vol11/iss1/8', 'https://scholarship.claremont.edu/jhm/vol7/iss2/7', 'https://scholarship.claremont.edu/jhm/vol3/iss1/4', 'https://scholarship.claremont.edu/jhm/vol5/iss1/5', 'https://scholarship.claremont.edu/jhm/vol9/iss1/8', 'https://scholarship.claremont.edu/jhm/vol5/iss1/3', 'https://scholarship.claremont.edu/jhm/vol10/iss2/6', 'https://scholarship.claremont.edu/jhm/vol7/iss2/5', 'https://scholarship.claremont.edu/jhm/vol9/iss2/4', 'https://scholarship.claremont.edu/jhm/vol9/iss2/3', 'https://scholarship.claremont.edu/jhm/vol10/iss1/8', 'https://scholarship.claremont.edu/jhm/vol10/iss2/4', 'https://scholarship.claremont.edu/jhm/vol11/iss1/6', 'https://scholarship.claremont.edu/jhm/vol10/iss1/9', 'https://scholarship.claremont.edu/jhm/vol9/iss2/7', 'https://scholarship.claremont.edu/jhm/vol8/iss1/7', 'https://scholarship.claremont.edu/jhm/vol9/iss1/4', 'https://scholarship.claremont.edu/jhm/vol7/iss2/6', 'https://scholarship.claremont.edu/jhm/vol7/iss2/4', 'https://scholarship.claremont.edu/jhm/vol5/iss2/3', 'https://scholarship.claremont.edu/jhm/vol6/iss1/9', 'https://scholarship.claremont.edu/jhm/vol7/iss2/9', 'https://scholarship.claremont.edu/jhm/vol5/iss1/4', 'https://scholarship.claremont.edu/jhm/vol9/iss1/5', 'https://scholarship.claremont.edu/jhm/vol7/iss1/7', 'https://scholarship.claremont.edu/jhm/vol8/iss2/7', 'https://scholarship.claremont.edu/jhm/vol8/iss1/5', 'https://scholarship.claremont.edu/jhm/vol11/iss1/4', 'https://scholarship.claremont.edu/jhm/vol9/iss1/9', 'https://scholarship.claremont.edu/jhm/vol7/iss2/8', 'https://scholarship.claremont.edu/jhm/vol8/iss2/8', 'https://scholarship.claremont.edu/jhm/vol10/iss1/3']\n",
      "87 new article found!\n",
      "51.72%\r"
     ]
    }
   ],
   "source": [
    "filename = 'journal_Journal_of_Humanistic_Mathematics_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "\n",
    "# save data\n",
    "df.to_pickle(os.path.join(datadir, filename))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['keyword'].values.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
