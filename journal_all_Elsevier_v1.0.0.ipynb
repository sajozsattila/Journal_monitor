{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.set_window_size(1920, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(journal, filename ):\n",
    "    start_url = \"https://www.sciencedirect.com/search\"\n",
    "    driver.get(start_url)\n",
    "    print(driver.current_url)\n",
    "    sleep(4)\n",
    "    \n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [], 'journal': [] })\n",
    "    try:\n",
    "        olddf = pd.read_csv(filename, sep=\"|\")\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # set journal\n",
    "    publisher = driver.find_element_by_id(\"pub\")\n",
    "    publisher.send_keys(journal)\n",
    "    sleep(4)\n",
    "    # send request\n",
    "    found = False\n",
    "    for sl in driver.find_elements_by_class_name('search-row'):\n",
    "        for search in sl.find_elements_by_tag_name('button'):\n",
    "            if search.text == \"Search\":\n",
    "                search.click()\n",
    "                sleep(5)\n",
    "                found = True\n",
    "        if found:\n",
    "            break    \n",
    "    print(driver.current_url)\n",
    "    \n",
    "    articles = []\n",
    "    def pagefind():\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # found the Article\n",
    "        for h5 in soup.find_all(\"h2\"):\n",
    "            # get link \n",
    "            for a in h5.find_all(\"a\"):        \n",
    "                articles.append(\"https://www.sciencedirect.com\"+a['href'])\n",
    "    \n",
    "        # try to find modal and close it\n",
    "        for modal in driver.find_elements_by_class_name('modal-close-button'):\n",
    "            modal.click()\n",
    "            sleep(2)\n",
    "        \n",
    "        # avoid feedback bottom\n",
    "        driver.set_window_size(1920, 1080)\n",
    "    \n",
    "        for pl in driver.find_elements_by_class_name('pagination-link'):        \n",
    "            for a in pl.find_elements_by_tag_name('a'):\n",
    "                if a.text == \"next\":\n",
    "                    a.click()            \n",
    "                    sleep(2)\n",
    "                    return True\n",
    "\n",
    "    # itterate over search pages on the Next bottom\n",
    "    nextpage = True\n",
    "    offset = 0\n",
    "    while  nextpage:\n",
    "        nextpage = pagefind()\n",
    "        print(driver.current_url, end='\\r')\n",
    "        \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articles = list( set(articles).difference(set(oldarticles) ))\n",
    "    print(len(articles), \"new article found!\")    \n",
    "    \n",
    "    df = pd.DataFrame({'url': articles, 'journal': journal })\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/journal_Climate_Risk_Management_'+version+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getarticledetails(df, olddf, filename):\n",
    "\n",
    "    articles = df['url'].values\n",
    "    \n",
    "    titles = [ None for _ in range(len(df))]\n",
    "    abstracts = [ None for _ in range(len(df))]\n",
    "    writers = [ None for _ in range(len(df))]\n",
    "    dates = [ None for _ in range(len(df))]\n",
    "    dois = [  None for _ in range(len(df)) ]\n",
    "    keywords = [  None for _ in range(len(df)) ]\n",
    "\n",
    "    for idx in range(len(articles)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(df),2))+\"%\" , end='\\r')\n",
    "    \n",
    "        url = df.iloc[idx]['url']\n",
    "        driver.get(url)\n",
    "        sleep(2)\n",
    "        content = driver.page_source\n",
    "    \n",
    "        page_soup = BeautifulSoup(content, features=\"lxml\", from_encoding='utf-8') \n",
    "    \n",
    "        # abstract\n",
    "        try:\n",
    "            abstracts[idx] = page_soup.find(\"p\", {'id':['sp0005']}).text\n",
    "        except:\n",
    "            print(\"Error: \", url)\n",
    "            continue\n",
    "\n",
    "        # title, doi and publication date\n",
    "        for m in page_soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == \"citation_title\":\n",
    "                    titles[idx] = m['content']            \n",
    "                elif m['name'] == \"citation_online_date\":\n",
    "                    # change to ISO dateformat\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                elif m['name'] == \"citation_doi\":\n",
    "                    dois[idx] = m['content']    \n",
    "\n",
    "        # keywords\n",
    "        key = []\n",
    "        try:\n",
    "            for kdiv in page_soup.find(\"div\", {'class':['keywords-section']}).find_all(\"div\", {'class':['keyword']}):\n",
    "                key.append(kdiv.get_text())\n",
    "        except:\n",
    "            pass\n",
    "        if len(key) > 0:\n",
    "            keywords[idx] =  \"#\".join(key)\n",
    "                    \n",
    "        #  language\n",
    "        try:\n",
    "            for script in page_soup.find_all(\"script\"):\n",
    "                if re.search(\"abstracts\", str(script.string)):\n",
    "                    data = json.loads(script.string)\n",
    "                    language = data['abstracts']['content'][0]['$']['lang']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # writers\n",
    "        writter = []\n",
    "        aut = driver.find_element_by_id('author-group')\n",
    "        for auta in aut.find_elements_by_class_name(\"workspace-trigger\"):\n",
    "            auta.click()\n",
    "            sleep(1)\n",
    "            # refresh page\n",
    "            content = driver.page_source\n",
    "            page_soup = BeautifulSoup(content, features=\"lxml\", from_encoding='utf-8')\n",
    "            auta.click()\n",
    "            # refresh page\n",
    "            content = driver.page_source\n",
    "            page_soup = BeautifulSoup(content, features=\"lxml\", from_encoding='utf-8')            \n",
    "            for autdiv in page_soup.find_all(\"div\", {'class':['WorkspaceAuthor']}):\n",
    "                givenname = \"\"\n",
    "                for aut_span in autdiv.find_all(\"span\", {'class':['given-name']}):\n",
    "                    givenname = aut_span.text\n",
    "                    break\n",
    "                familyname = \"\"\n",
    "                for aut_span in autdiv.find_all(\"span\", {'class':['surname']}):\n",
    "                    familyname = aut_span.text\n",
    "                    break\n",
    "                institude = \"\"\n",
    "                for aut_span in autdiv.find_all(\"div\", {'class':['affiliation']}):\n",
    "                    institude = aut_span.text\n",
    "                    break\n",
    "                writter.append(familyname+\", \"+givenname + \"--\"+institude)\n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        \n",
    "\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # merge with old df\n",
    "    df = pd.concat([df, olddf])\n",
    "    \n",
    "    # save data\n",
    "    df.to_csv(filename, sep=\"|\", index=False)\n",
    "                \n",
    "    test = pd.read_csv(filename, sep=\"|\")\n",
    "    print(test.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = [\"Climate Risk Management\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sciencedirect.com/search\n",
      "https://www.sciencedirect.com/search?pub=Climate%20Risk%20Management\n",
      "273 new article found!ect.com/search?pub=Climate%20Risk%20Management&offset=250\n",
      "0.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:220: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108000075\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096315000406\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096318300287\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108010022\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108120019\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108010010\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096314000308\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096318301839\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096318302110\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096317300475\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108000038\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096314000278\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096315000066\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108000051\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096317301663\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096317301407\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096317301328\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096316300092\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096320300085\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096316300948\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B978012397310800004X\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096315000091\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096319300890\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096315000182\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108000014\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108180017\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096316300018\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B978012397310804001X\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096317300748\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096320300048\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096318300895\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108000026\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/B9780123973108000063\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096314000345\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096314000187\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096316300365\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096314000023\n",
      "Error:  https://www.sciencedirect.com/science/article/pii/S2212096315000339\n",
      "                                                   url  \\\n",
      "268  https://www.sciencedirect.com/science/article/...   \n",
      "269  https://www.sciencedirect.com/science/article/...   \n",
      "270  https://www.sciencedirect.com/science/article/...   \n",
      "271  https://www.sciencedirect.com/science/article/...   \n",
      "272  https://www.sciencedirect.com/science/article/...   \n",
      "\n",
      "                     journal  \\\n",
      "268  Climate Risk Management   \n",
      "269  Climate Risk Management   \n",
      "270  Climate Risk Management   \n",
      "271  Climate Risk Management   \n",
      "272  Climate Risk Management   \n",
      "\n",
      "                                                 title  \\\n",
      "268                                                NaN   \n",
      "269  Management options for rainfed chickpea (Cicer...   \n",
      "270  Multiple methods for multiple futures: Integra...   \n",
      "271  Fisheries management responses to climate chan...   \n",
      "272  A stress test for climate change impacts on wa...   \n",
      "\n",
      "                          dois  \\\n",
      "268                        NaN   \n",
      "269  10.1016/j.crm.2016.12.003   \n",
      "270  10.1016/j.crm.2017.07.002   \n",
      "271  10.1016/j.crm.2015.09.001   \n",
      "272  10.1016/j.crm.2020.100222   \n",
      "\n",
      "                                              abstract  \\\n",
      "268                                                NaN   \n",
      "269  Chickpea (Cicer arietinum L.) is one of the im...   \n",
      "270  Scenario planning helps managers incorporate c...   \n",
      "271                                                NaN   \n",
      "272  Since the impacts of climate change will be fe...   \n",
      "\n",
      "                                                writer publishdate  \\\n",
      "268                                                NaN         NaN   \n",
      "269  Mohammed, Adem--College of Agricultural and En...  2016-12-22   \n",
      "270  Symstad, Amy J.--U.S. Geological Survey, North...  2017-07-19   \n",
      "271  Th√∏gersen, Thomas--Technical University of Den...  2015-10-26   \n",
      "272  Verbist, K.M.J.--UNESCO International Hydrolog...  2020-03-23   \n",
      "\n",
      "                                              keywords  \n",
      "268                                                NaN  \n",
      "269  Chickpea#Climate change#CROPGRO-model#Drought#...  \n",
      "270  Adaptive management#Climate change adaptation#...  \n",
      "271  Climate change#Bio economic model#Management#F...  \n",
      "272  Risk management#Water security#Climate change#...  \n"
     ]
    }
   ],
   "source": [
    "for jidx in range(len(journals)):\n",
    "    filename = filename = 'data/journal_'+ journals[jidx].replace(\" \", \"_\") +'_'+version+'.csv'\n",
    "    \n",
    "    # search for articles\n",
    "    df, olddf = gedjournalarticles( journals[jidx], filename )\n",
    "    \n",
    "    # get the articles details\n",
    "    getarticledetails(df, olddf, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
