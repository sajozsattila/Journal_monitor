{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.get(\"http://periodicos.pucminas.br/index.php/abakos/issue/archive\")\n",
    "    \n",
    "    # get all of the issue\n",
    "    def issues():\n",
    "        issues = []\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for div in soup.find_all('div', {'class': 'obj_issue_summary'}):\n",
    "            for a in div.find_all('a', {'class': 'title'}):                \n",
    "                issues.append(a['href'])        \n",
    "        \n",
    "        return  issues    \n",
    "    allissue = issues()\n",
    "    allissue = list(set(allissue))\n",
    "    \n",
    "    def articles():\n",
    "        articles = []\n",
    "        nexpage = False\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # We collect the texts under the Articles         \n",
    "        # get the 'Artilces' h2\n",
    "        for div in soup.find_all('div', {'class': 'obj_article_summary'}):\n",
    "            for title in div.find_all('div', {'class': 'title'}):\n",
    "                for a in title.find_all('a'):\n",
    "                    articles.append(a['href'])\n",
    "                    \n",
    "            \n",
    "        return  articles\n",
    "    \n",
    "    # iterate over isses and get article links\n",
    "    articellinks = []\n",
    "    for issue in allissue:\n",
    "        driver.get(issue)\n",
    "        sleep(2)\n",
    "        print(driver.current_url, end='\\r')         \n",
    "        thispagearticles = articles()\n",
    "        articellinks = articellinks + thispagearticles        \n",
    "    articellinks = list(set(articellinks))\n",
    "    \n",
    "    # filter out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articellinks = list( set(articellinks).difference(set(oldarticles) ))\n",
    "    print(len(articellinks), \"new article found!\")\n",
    "    \n",
    "    # iterate over article \n",
    "    urls = [ None for _ in range(len(articellinks))]\n",
    "    titles = [ None for _ in range(len(articellinks))]\n",
    "    abstracts = [ None for _ in range(len(articellinks))]\n",
    "    writers = [ None for _ in range(len(articellinks))]\n",
    "    dates = [ None for _ in range(len(articellinks))]\n",
    "    dois = [  None for _ in range(len(articellinks)) ]\n",
    "    keywords = [  None for _ in range(len(articellinks)) ]\n",
    "    languages = [ None for _ in range(len(articellinks)) ]\n",
    "    for idx in range(len(articellinks)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(articellinks),2))+\"%\" , end='\\r')\n",
    "        url =  articellinks[idx]\n",
    "        # load page\n",
    "        driver.get(articellinks[idx])\n",
    "        sleep(3)        \n",
    "        urls[idx] = driver.current_url\n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # process metadata\n",
    "        keyword = []\n",
    "        writter = []\n",
    "        abstract = \"\"\n",
    "        isarticle = True\n",
    "        for m in soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == \"DC.Type.articleType\":\n",
    "                    if not re.search('Full papers', m['content']) and re.search('Full papers', m['content']):\n",
    "                        isarticle = False\n",
    "                if m['name'] == \"keywords\":\n",
    "                    keyword = [ x.strip() for x in m['content'].split(\",\") ]\n",
    "                if m['name'] == \"DC.Date.issued\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'DC.Creator.PersonalName':\n",
    "                    writter.append(m['content'])\n",
    "                if m['name'] == 'citation_doi':\n",
    "                    dois[idx] = m['content']\n",
    "                if m['name'] == 'DC.Description':\n",
    "                    abstracts[idx] = m['content']\n",
    "                if m['name'] == 'citation_language':\n",
    "                    languages[idx] = m['content']\n",
    "                if m['name'] == 'DC.Title':\n",
    "                     titles[idx] = m['content']\n",
    "        # if it is not Article (like editor or Communication) we will drop\n",
    "        if not isarticle:\n",
    "            abstracts[idx] = None\n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        if len(keyword) > 0:\n",
    "            keywords[idx] =  \"#\".join(keyword)\n",
    "        # filter out DOI from title\n",
    "        print(dois[idx],  titles[idx])\n",
    "        if dois[idx] is not None and titles[idx] is not None:\n",
    "            if re.search(dois[idx], titles[idx]):\n",
    "                titles[idx] = titles[idx].replace(\"  \", \" \")\n",
    "                titles[idx] = titles[idx].replace(\" DOI - \"+dois[idx], '')\n",
    "                titles[idx] = titles[idx].replace(\" DOI - \"+dois[idx].upper(), '')\n",
    "                titles[idx] = titles[idx].replace(\" - DOI \"+dois[idx], '')\n",
    "                titles[idx] = titles[idx].replace(\" - DOI \"+dois[idx].upper(), '')\n",
    "                titles[idx] = titles[idx].replace(\" DOI \"+dois[idx], '')\n",
    "                titles[idx] = titles[idx].replace(\" DOI \"+dois[idx].upper(), '')\n",
    "                titles[idx] = titles[idx].replace(dois[idx], '')\n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': urls, \n",
    "                       'journal_title': \"Abak√≥s\", \n",
    "                       'journal_eissn': \"2316-9451\",\n",
    "                       'journal_pissn': '',\n",
    "                       'category': \"Information technology\",                                              \n",
    "                      })\n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    df['language'] = languages\n",
    "    \n",
    "    # drop where is no abstract\n",
    "    df = df[ df['abstract'].notna()]\n",
    "    # drop where title is Editorial\n",
    "    df = df[df['title'] != 'Editorial']    \n",
    "    \n",
    "    # check there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'journal_Abakos_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "\n",
    "# save data\n",
    "df.to_pickle(os.path.join(datadir, filename))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['title'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].values.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
