{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading \n",
    "from queue import Queue\n",
    "import feedparser\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.set_window_size(1920, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jname = \"iop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListeningRSS(threading.Thread):\n",
    "    def __init__(self, job):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.stoprequest = threading.Event()\n",
    "        self.mur2job = job\n",
    "        self.memory = {}\n",
    "        self.memoryname = 'data/journal_all_'+jname+'_feeds.pickle'\n",
    "        try:\n",
    "            with open(self.memoryname, 'rb') as handle:\n",
    "                self.memory = pickle.load(handle)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.feeds = pd.read_csv('journal_all_iop_rss.csv', sep=\"|\")\n",
    "        self.feeds = self.feeds.to_dict('records')\n",
    "        \n",
    "    def join(self, timeout=None):\n",
    "        self.stoprequest.set()\n",
    "        super(ListeningRSS, self).join(timeout)\n",
    "    def run(self):\n",
    "        print(self.memory)\n",
    "        while True:\n",
    "            for feedl in self.feeds:\n",
    "                feedurl = feedl['url']\n",
    "                feed = feedparser.parse(feedurl)\n",
    "                # print(feed)\n",
    "                # add feed if it is not in the dictionary\n",
    "                if feedurl not in self.memory.keys():\n",
    "                    self.memory[feedurl] = { 'lasturl': '' }\n",
    "                # check for new feed\n",
    "                newfeedIdx = 0 \n",
    "                for item in feed['items']:\n",
    "                    journal = item['prism_publicationname']\n",
    "                    if journal not in self.memory.keys():\n",
    "                        self.memory[journal] = {}\n",
    "                    \n",
    "                    # get what is possible from the RSS\n",
    "                    url = item['link']\n",
    "                    \n",
    "                    # check url is in the last on or not\n",
    "                    if url not in self.memory[journal].keys():\n",
    "                        self.memory[journal][url] = datetime.datetime.now() \n",
    "                        # keep memory under control, so drop last observation\n",
    "                        if len(self.memory[journal].keys()) > 200:\n",
    "                            # drop oldest\n",
    "                            keys = dict(sorted((value, key) for (key,value) in self.memory[journal].items()))\n",
    "                            key = keys[list(keys.keys())[0]]\n",
    "                            self.memory[journal].pop( key )\n",
    "                        # pickle the memory\n",
    "                        with open(self.memoryname, 'wb') as handle:\n",
    "                            pickle.dump(self.memory, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    else:\n",
    "                        continue                            \n",
    "                    \n",
    "                    title = item['title']\n",
    "                    doi = item['prism_doi']\n",
    "                    # visit the article page to get the other information\n",
    "                    try:\n",
    "                        driver.get(url)\n",
    "                    except:\n",
    "                        print(\"Error in url:\", url)\n",
    "                        continue\n",
    "                    content = driver.page_source\n",
    "                    # load the page content in BeautifulSoup\n",
    "                    page_soup = BeautifulSoup(content, features=\"lxml\")\n",
    "                    # abstract\n",
    "                    abstract = \"\"\n",
    "                    for abstdiv in page_soup.find_all(\"div\", {'class':['wd-jnl-art-abstract']}):\n",
    "                        for ap in abstdiv.find_all(\"p\"):\n",
    "                            abstract = abstract + (\" \".join(ap.get_text().splitlines()\n",
    "                                                           ).replace(\"\\xa0\", \" \") ).replace(\"Abstract\", '')\n",
    "                    if len(abstract) == 0:\n",
    "                        print(\"No abstract\", url)\n",
    "                        continue\n",
    "                    # make some action to not look as boot\n",
    "                    element = driver.find_element_by_tag_name('body')\n",
    "                    action = webdriver.ActionChains(driver)\n",
    "                    action.move_to_element(element)\n",
    "                    try:\n",
    "                        action.move_by_offset( random.uniform(0, 120),  random.uniform(200, 500)).perform()\n",
    "                    except:\n",
    "                        pass\n",
    "                    sleep( random.uniform(20, 120) )                    \n",
    "                    try:\n",
    "                        action.move_by_offset( random.uniform(0, 120),  random.uniform(-100, 500)).perform()\n",
    "                    except:\n",
    "                        pass\n",
    "                    element.send_keys(Keys.END)\n",
    "\n",
    "                    # keywords\n",
    "                    keywords = \"\"\n",
    "                    date = \"\"\n",
    "                    language = \"\"\n",
    "                    writers = []\n",
    "                    # this flag to catch the author institute\n",
    "                    thiswriter = False\n",
    "                    for m in page_soup.find_all(\"meta\"):\n",
    "                        if m.has_attr(\"name\"):\n",
    "                            if m['name'] == \"dc.Subject\":\n",
    "                                keywords = m['content'].replace(\";\", \"#\")  \n",
    "                            elif m['name'] == \"citation_online_date\":\n",
    "                                try:\n",
    "                                    date = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                                except Exception as e:\n",
    "                                    try:\n",
    "                                        date = datetime.datetime.strptime(m['content'], \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "                                    except Exception as e:\n",
    "                                        print(\"Datetime error:\", m['content'], \"url:\", url)\n",
    "                            elif m['name'] == \"citation_language\":                                \n",
    "                                language = m['content']\n",
    "                            elif  m['name'] == 'citation_author':\n",
    "                                writer = m['content']\n",
    "                                thiswriter = True\n",
    "                                writers.append(writer)\n",
    "                            elif  m['name'] == 'citation_author_institution' and thiswriter:\n",
    "                                writers[-1] = writers[-1]+\"--\"+ m['content']\n",
    "\n",
    "                            if not m['name'] == 'citation_author':\n",
    "                                thiswriter = False\n",
    "                    writers =  \"#\".join(writers)                      \n",
    "                    \n",
    "                    # data dict\n",
    "                    datadict = {\n",
    "                        'journal_title': journal,\n",
    "                        'url': url,\n",
    "                        'title': title,\n",
    "                        'writer': writers,\n",
    "                        'doi': doi,\n",
    "                        'abstract': abstract,\n",
    "                        'keyword': keywords,\n",
    "                        'publishdate': date,\n",
    "                        'language': language,\n",
    "                        'journal_eissn': feedl['eissn'],\n",
    "                        'journal_pissn': feedl['pissn'],\n",
    "                        'category': feedl['category']\n",
    "                    }\n",
    "                    \n",
    "                    # send data to loader\n",
    "                    self.mur2job.put(datadict)\n",
    "                    \n",
    "                    # \n",
    "                    sleep(random.uniform(150, 450))\n",
    "                    \n",
    "            # check every 6-12 hour \n",
    "            print(\"Wait for next run\")\n",
    "            sleep( random.uniform(21600, 43200) )\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_mur2(threading.Thread):\n",
    "    def __init__(self, job):\n",
    "        self.filename = 'data/journal_all_'+jname+'.pandas'\n",
    "        threading.Thread.__init__(self)\n",
    "        self.stoprequest = threading.Event()\n",
    "        self.job = job\n",
    "        self.df =  pd.DataFrame({'url': [], 'journal_title': [],\n",
    "                                'title': [], 'writer': [], \n",
    "                                'doi': [], 'abstract': [],\n",
    "                                'keyword': [], 'publishdate': [],\n",
    "                                'language': [],\n",
    "                                'category': [],\n",
    "                                'journal_eissn': [], 'journal_pissn': []})\n",
    "        # try to read old df\n",
    "        try:\n",
    "            self.df = pd.read_pickle(self.filename)\n",
    "            print(self.df.tail())\n",
    "        except:\n",
    "            pass\n",
    "    def join(self, timeout=None):\n",
    "        self.stoprequest.set()\n",
    "        super(Feed_mur2, self).join(timeout)\n",
    "    def run(self):\n",
    "        while True:\n",
    "            task = None\n",
    "            try:\n",
    "                task = self.job.get(True, 0.001) \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            # save to dataframe at the moment\n",
    "            self.df = self.df.append(dict(zip(self.df.columns,\n",
    "                                    [\n",
    "                                        task['url'], task['journal_title'],\n",
    "                                        task['title'], task['writer'], \n",
    "                                        task['doi'], task['abstract'],\n",
    "                                        task['keyword'], task['publishdate'],\n",
    "                                        task['language'], \n",
    "                                        task['category'],\n",
    "                                        task['journal_eissn'], task['journal_pissn']\n",
    "                                    ])), ignore_index=True)\n",
    "            \n",
    "            # save\n",
    "            self.df.to_pickle(self.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threadLock = threading.Lock()\n",
    "threads = []\n",
    "\n",
    "# queue for the jobs\n",
    "jobs = Queue(maxsize=0)\n",
    " \n",
    "listenerRss = ListeningRSS(jobs)\n",
    "listenerRss.start()\n",
    "threads.append(listenerRss)\n",
    " \n",
    "feedmur2 = Feed_mur2(jobs)\n",
    "feedmur2.start()\n",
    "threads.append(feedmur2)\n",
    " \n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    " \n",
    " \n",
    "print(\"Exiting Main Thread\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
