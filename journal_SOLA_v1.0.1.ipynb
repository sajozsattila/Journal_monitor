{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.1\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.get(\"https://www.jstage.jst.go.jp/browse/sola/list/-char/en\")\n",
    "    \n",
    "    # get all of the issue\n",
    "    def issues():\n",
    "        issues = []\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for a in soup.find_all('a', {'class': 'bluelink-style'}):\n",
    "            # if it is this journal\n",
    "            if re.search(\"Volume\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                issues.append(a['href'])        \n",
    "        \n",
    "        return  issues    \n",
    "    allissue = issues()\n",
    "    allissue = list(set(allissue))\n",
    "    \n",
    "    def articles():\n",
    "        articles = []\n",
    "        nexpage = False\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for a in soup.find_all('a'):\n",
    "            # if it is this journal\n",
    "            if re.search(\"^https://doi.org\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                articles.append(a['href'])\n",
    "        # check there is new issue\n",
    "        for a in driver.find_elements_by_name('a'):\n",
    "            if '>' == a.getText():\n",
    "                nexpage = True\n",
    "                # set to nexpage\n",
    "                a.click()\n",
    "                sleep(3)        \n",
    "        return  articles, nexpage\n",
    "    \n",
    "    # iterate over isses and get article links\n",
    "    articellinks = []\n",
    "    for issue in allissue:\n",
    "        driver.get(issue)\n",
    "        sleep(2)\n",
    "        print(driver.current_url, end='\\r') \n",
    "        # get the issue article\n",
    "        nextpage = True\n",
    "        allissue = []\n",
    "        while nextpage:\n",
    "            thispagearticles, nextpage = articles()\n",
    "            articellinks = articellinks + thispagearticles\n",
    "            print(driver.current_url, end='\\r')        \n",
    "    articellinks = list(set(articellinks))\n",
    "    \n",
    "    # filter out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articellinks = list( set(articellinks).difference(set(oldarticles) ))\n",
    "    print(len(articellinks), \"new article found!\")\n",
    "    \n",
    "    # iterate over article \n",
    "    urls = [ None for _ in range(len(articellinks))]\n",
    "    titles = [ None for _ in range(len(articellinks))]\n",
    "    abstracts = [ None for _ in range(len(articellinks))]\n",
    "    writers = [ None for _ in range(len(articellinks))]\n",
    "    dates = [ None for _ in range(len(articellinks))]\n",
    "    dois = [  None for _ in range(len(articellinks)) ]\n",
    "    keywords = [  None for _ in range(len(articellinks)) ]\n",
    "    for idx in range(len(articellinks)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(articellinks),2))+\"%\" , end='\\r')\n",
    "        url =  articellinks[idx]\n",
    "        # load page\n",
    "        driver.get(articellinks[idx])\n",
    "        sleep(3)        \n",
    "        urls[idx] = driver.current_url\n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # process metadata\n",
    "        keyword = []\n",
    "        writter = []\n",
    "        notpaper = False\n",
    "        for m in soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == 'title':\n",
    "                    titles[idx] = m['content']\n",
    "                if m['name'] == \"citation_keywords\":\n",
    "                    keyword.append( m['content'] )\n",
    "                if m['name'] == \"citation_online_date\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'citation_author':\n",
    "                    writter.append(m['content'])\n",
    "                if re.search('citation_doi', m['name']):\n",
    "                    dois[idx] = m['content']                    \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        if len(keyword) > 0:\n",
    "            keywords[idx] =  \"#\".join(keyword)\n",
    "            \n",
    "        # get abstract\n",
    "        abstract = \"\"\n",
    "        if notpaper:\n",
    "            continue\n",
    "        else:\n",
    "            for div in soup.find_all(id='article-overiew-abstract-wrap'):\n",
    "                for c in div.find_all(['p', 'script']):\n",
    "                    if c.name == \"p\":                    \n",
    "                        for tag in  c.contents:\n",
    "                            if re.search(\"<span\", str(tag)):\n",
    "                                continue\n",
    "                            else:\n",
    "                                match = re.match(\"<script.+>\", str(tag))\n",
    "                                if match is not None:\n",
    "                                    abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                                else:\n",
    "                                    abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "                # if still not abstract it is possible it is not in p\n",
    "                if len(abstract) == 0:\n",
    "                    for tag in div.contents:\n",
    "                        if re.search(\"<span\", str(tag)) or re.search(\"<h3\", str(tag)) :\n",
    "                            continue\n",
    "                        else:\n",
    "                            match = re.match(\"<script.+>\", str(tag))\n",
    "                            if match is not None:\n",
    "                                abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                            else:\n",
    "                                abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "            abstracts[idx] = abstract       \n",
    "        \n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': urls, \n",
    "                       'journal_title': \"SOLA\", \n",
    "                       'journal_eissn': \"1349-6476\",\n",
    "                       'journal_pissn': '',\n",
    "                       'category': \"Meteorology. Climatology\",                                              \n",
    "                      })\n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # drop where is no abstract\n",
    "    df = df[ df['abstract'].notna()]\n",
    "    # drop where title is Editorial\n",
    "    df = df[df['title'] != 'Editorial']\n",
    "    # drop where title \"The SOLA Award in *\"\"\n",
    "    df = df[df.title.str.contains('^(?!The SOLA Award in)', regex= True, na=False)]\n",
    "    \n",
    "    \n",
    "    # check there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762 new article found!.go.jp/browse/sola/6/0/_contents/-char/enen\n",
      "3.15%\r"
     ]
    }
   ],
   "source": [
    "filename = 'journal_SOLA_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "\n",
    "# save data\n",
    "df.to_pickle(os.path.join(datadir, filename))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
