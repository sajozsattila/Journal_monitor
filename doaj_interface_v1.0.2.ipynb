{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.2\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Journals which we are searching for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "journals = pd.read_csv('doaj_journals.csv', sep=\"|\")\n",
    "journals.drop_duplicates(inplace=True)\n",
    "import re\n",
    "journals.tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the article details from DOAJ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findjournaldata(results):\n",
    "    publisher = None\n",
    "    eissn = None\n",
    "    pissn = None\n",
    "    title = None\n",
    "    doajid = None\n",
    "    categories = None\n",
    "    language = None\n",
    "    notfound = True\n",
    "    # iterate over the result and try to find where title exactly match\n",
    "    for result in results:\n",
    "            if result['bibjson']['title'].lower().strip() == journals.iloc[idx]['title'].lower().strip():\n",
    "                notfound = False\n",
    "                publisher = result['bibjson']['publisher']['name'] \n",
    "                if 'eissn' in result['bibjson'].keys():\n",
    "                    eissn = result['bibjson']['eissn'] \n",
    "                if 'pissn' in result['bibjson'].keys():\n",
    "                    pissn = result['bibjson']['pissn'] \n",
    "\n",
    "                title = result['bibjson']['title'] \n",
    "                doajid = result['id'] \n",
    "                categories =  \"#\".join(  [ s['term'].strip() for s in result['bibjson']['subject'] ] ) \n",
    "                language = \"#\".join(  [ s.strip() for s in result['bibjson']['language'] ] ) \n",
    "                \n",
    "                break\n",
    "    return notfound, publisher, eissn, pissn, title, doajid, categories, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from time import sleep\n",
    "\n",
    "publishers = []\n",
    "eissns = []\n",
    "pissns = []\n",
    "titles = []\n",
    "doajids = []\n",
    "categoriess = []\n",
    "languages = []\n",
    "for idx in range(len(journals)):\n",
    "    if 'doajid' not in journals.columns or journals.iloc[idx]['doajid'] is None or not isinstance(journals.iloc[idx]['doajid'], str):\n",
    "        searchterm = urllib.parse.quote('bibjson.title:\"'+journals.iloc[idx]['title']+'\"')\n",
    "        searchterm = 'https://doaj.org/api/v2/search/journals/'+searchterm\n",
    "        x = requests.get(searchterm)\n",
    "        notfound = True\n",
    "        if x.status_code == 200:\n",
    "            # iterate over pages\n",
    "            jsonresult =  x.json()\n",
    "            if 'total' in jsonresult.keys() :\n",
    "                for page in range(int(jsonresult['total'])):\n",
    "                    page = page+1\n",
    "                    if 'results' in jsonresult.keys():                \n",
    "                        notfound, publisher, eissn, pissn, title, doajid, categories, language = findjournaldata(jsonresult['results'])\n",
    "                        if not notfound:\n",
    "                            publishers.append(publisher)\n",
    "                            eissns.append(eissn)\n",
    "                            pissns.append(pissn)\n",
    "                            titles.append(title)\n",
    "                            doajids.append(doajid)\n",
    "                            categoriess.append(categories)\n",
    "                            languages.append(language)\n",
    "                            break\n",
    "                        else:\n",
    "                            if jsonresult['total'] > 1:\n",
    "                                # go to next page\n",
    "                                x = requests.get(searchterm+\"?page=\"+str(page)+\"&pageSize=10\")\n",
    "                                print(searchterm+\"?page=\"+str(page)+\"&pageSize=10\")\n",
    "                                if x.status_code == 200:\n",
    "                                    jsonresult =  x.json()\n",
    "                                else:\n",
    "                                    break\n",
    "                \n",
    "       \n",
    "        if notfound:\n",
    "            # test there is new page if yes go there        \n",
    "            print(\"Error:\", searchterm)\n",
    "            publishers.append(None)\n",
    "            eissns.append(None)\n",
    "            pissns.append(None)\n",
    "            titles.append(journals.iloc[idx]['title'])\n",
    "            doajids.append(None)\n",
    "            categoriess.append(None)\n",
    "            languages.append(None)\n",
    "        sleep(3)\n",
    "    else:\n",
    "        publishers.append(journals.iloc[idx]['publisher'])\n",
    "        eissns.append(journals.iloc[idx]['eissn'])\n",
    "        pissns.append(journals.iloc[idx]['pissn'])\n",
    "        titles.append(journals.iloc[idx]['title'])\n",
    "        doajids.append(journals.iloc[idx]['doajid'])\n",
    "        categoriess.append(journals.iloc[idx]['categories'])\n",
    "        languages.append(journals.iloc[idx]['language'])\n",
    "    \n",
    "# enrich journals\n",
    "journals['publisher'] = publishers\n",
    "journals['eissn'] = eissns\n",
    "journals['pissn'] = pissns\n",
    "journals['title'] = titles\n",
    "journals['doajid'] = doajids\n",
    "journals['categories'] = categoriess\n",
    "journals['language'] = languages\n",
    "\n",
    "journals.to_csv('doaj_journals.csv', sep=\"|\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(journals.head())\n",
    "journals.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the articles for the Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findarticledata(results):\n",
    "    \n",
    "    # iterrate over Articles\n",
    "    url = []\n",
    "    title = []\n",
    "    doi = []\n",
    "    abstract  = []\n",
    "    writers = []\n",
    "    publishdate = []\n",
    "    keyword = []\n",
    "    for result in results:\n",
    "\n",
    "        # url\n",
    "        urlfound = False\n",
    "        for l in result['bibjson'][\"link\"]:\n",
    "            if l['type'] == \"fulltext\":\n",
    "                urlfound = True\n",
    "                url.append(  l['url'].replace(\"\\n\", \"\") )\n",
    "                break\n",
    "        if not urlfound:\n",
    "            # if there no url we do not interest about the Article\n",
    "            url.append(None)\n",
    "            title.append(None)\n",
    "            doi.append(None)\n",
    "            abstract.append( None )\n",
    "            writers.append(None)\n",
    "            publishdate.append( None )\n",
    "            keyword.append( None )\n",
    "            continue\n",
    "                \n",
    "            \n",
    "        # title\n",
    "        if \"title\" in  result['bibjson']:\n",
    "            title.append( result['bibjson'][\"title\"].replace(\"\\n\", \"\") )\n",
    "        else:\n",
    "            # if there no title we do not interest about the Article\n",
    "            title.append(None)\n",
    "            doi.append(None)\n",
    "            abstract.append( None )\n",
    "            writers.append(None)\n",
    "            publishdate.append( None )\n",
    "            keyword.append( None )\n",
    "            continue\n",
    "        \n",
    "        # doi\n",
    "        founddoi = False\n",
    "        for ide in result['bibjson']['identifier']:\n",
    "            if ide[\"type\"].lower() == \"doi\":\n",
    "                founddoi = True\n",
    "                doi.append( ide['id'].replace(\"\\n\", \"\") )\n",
    "                break\n",
    "        if not founddoi:\n",
    "            doi.append(None)\n",
    "        \n",
    "        # abstract\n",
    "        if \"abstract\" in  result['bibjson']:\n",
    "            abstract.append( result['bibjson'][\"abstract\"].replace(\"\\n\", '<br>') )\n",
    "        else:\n",
    "            # if there no abstract we do not interest about the Article\n",
    "            abstract.append( None )\n",
    "            writers.append(None)\n",
    "            publishdate.append( None )\n",
    "            keyword.append( None )\n",
    "            continue\n",
    "        \n",
    "        # writer\n",
    "        writer = \"\"\n",
    "        for w in result['bibjson'][\"author\"]:\n",
    "            try:\n",
    "                writer = writer + \"#\" + w['name'].replace(\"\\n\", \"\")\n",
    "                if 'affiliation' in w.keys():\n",
    "                    writer = writer + \"--\" + w['affiliation'].replace(\"\\n\", \"\")\n",
    "                if 'orcid_id' in w.keys():\n",
    "                    writer = writer + \"---\" + w['orcid_id'].replace(\"\\n\", \"\")          \n",
    "            except:\n",
    "                pass\n",
    "        if len(writer) > 0:\n",
    "            writers.append(writer)\n",
    "        else:\n",
    "            writers.append(None)\n",
    "            publishdate.append( None )\n",
    "            keyword.append( None )\n",
    "            continue\n",
    "        \n",
    "        # publishdate\n",
    "        if \"created_date\" in result.keys():\n",
    "            publishdate.append( result[\"created_date\"].replace(\"\\n\", \"\") )\n",
    "        else:\n",
    "            publishdate.append( None )\n",
    "        \n",
    "        # keywords\n",
    "        # we put there a filter as if there is too much keyword the are not informatiom\n",
    "        if \"keywords\" in result['bibjson'] and len(result['bibjson'][\"keywords\"]) < 15 :\n",
    "            keytext = \"\"            \n",
    "            for k in result['bibjson'][\"keywords\"]:\n",
    "                if len(k) < 100:\n",
    "                    keytext = keytext + \"#\" + k.replace(\"\\n\", \"\")\n",
    "            if len(keytext) == 0:\n",
    "                keyword.append( None )\n",
    "            else:\n",
    "                keyword.append( keytext )\n",
    "        else:\n",
    "            keyword.append( None )\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'doi': doi,\n",
    "        'abstract': abstract,\n",
    "        'writer': writers,\n",
    "        'publishdate': publishdate,\n",
    "        'keyword': keyword,\n",
    "                      })\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itterate_over_results(searchterm, jsonresult, journals, journaldf, desc=True):\n",
    "    if 'total' in jsonresult.keys() :                            \n",
    "            for page in range(pagenumber):\n",
    "                if 'results' in jsonresult.keys():   \n",
    "                    thispage = findarticledata(jsonresult['results'])\n",
    "                    thispage['journal_title'] = journals.iloc[idx]['title']\n",
    "                    thispage['journal_eissn'] = journals.iloc[idx]['eissn']\n",
    "                    thispage['journal_pissn'] = journals.iloc[idx]['pissn']\n",
    "                    thispage['category'] = journals.iloc[idx]['categories']\n",
    "                    \n",
    "                    journaldf = pd.concat([journaldf, thispage])                    \n",
    "                # go to othe next page\n",
    "                if jsonresult['total'] > 1 and page != pagenumber-1 :\n",
    "                    sleep(3)\n",
    "                    # go to next page\n",
    "                    x = None\n",
    "                    if desc:\n",
    "                        x = requests.get(searchterm+\"?page=\"+str(page+2)+\"&pageSize=100&sort=bibjson.year:desc\")\n",
    "                    else:\n",
    "                        x = requests.get(searchterm+\"?page=\"+str(page+2)+\"&pageSize=100&sort=bibjson.year:asc\")\n",
    "                    if x.status_code == 200:\n",
    "                        jsonresult =  x.json()\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "    return journaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# iterate over the journals\n",
    "for idx in range(len(journals)): \n",
    "    print(\"load:\", journals.iloc[idx]['title'], \"Ready: \", str(np.round(100*idx/len(journals),2))+\"%\", end='\\r')\n",
    "    # searchtext = 'journal:\"'+journals.iloc[idx]['title']+'\" AND _exists_:doi AND _exists_:abstract  AND _exists_: \"bibjson.author\" '\n",
    "    searchtext = 'journal:\"'+journals.iloc[idx]['title']+'\" AND _exists_:abstract  AND _exists_: \"bibjson.author\" '\n",
    "    # add issn\n",
    "    if isinstance(journals.iloc[idx]['eissn'], str):\n",
    "        searchtext = searchtext + ' AND issn: \"'+journals.iloc[idx]['eissn']+'\"'\n",
    "    else:\n",
    "        if isinstance(journals.iloc[idx]['pissn'], str):\n",
    "            searchtext = searchtext + ' AND issn: \"'+journals.iloc[idx]['pissn']+'\"'        \n",
    "    searchterm = urllib.parse.quote(searchtext)\n",
    "    searchterm = 'https://doaj.org/api/v2/search/articles/'+searchterm\n",
    "    x = requests.get(searchterm +'?page=1&pageSize=100&sort=bibjson.year:desc')    \n",
    "    notfound = True\n",
    "    \n",
    "    journaldf  = pd.DataFrame({\n",
    "        'url': [],\n",
    "        'title': [],\n",
    "        'doi': [],\n",
    "        'abstract': [],\n",
    "        'writer': [],\n",
    "        'publishdate': [],\n",
    "        'keyword': [],\n",
    "        'journal_title': [],\n",
    "        'journal_eissn': [],\n",
    "        'journal_pissn': [],\n",
    "        'category': []\n",
    "                      })\n",
    "    \n",
    "    if x.status_code == 200:\n",
    "        # is the dataset fit in the DOAJ 1000 Article filter?\n",
    "        reruninasc = False\n",
    "        # there is more than 2000 article in the Journal\n",
    "        middlepart = False\n",
    "        middlestart = None\n",
    "        middleend = None\n",
    "        # iterate over pages\n",
    "        jsonresult =  x.json()\n",
    "        if 'total' in jsonresult.keys() :\n",
    "            pagenumber = int(jsonresult['total']//100)+1\n",
    "            if jsonresult['total'] > 1000 :                \n",
    "                reruninasc = True\n",
    "            if jsonresult['total'] > 2000 :\n",
    "                # we need get Articles between\n",
    "                middlepart = True\n",
    "            journaldf = itterate_over_results(searchterm, jsonresult, journals, journaldf, desc=True)\n",
    "            # which youear finished the first 1000\n",
    "            middlestart = int(min(list(set(\n",
    "                [ x.split(\"-\")[0] for x  in journaldf['publishdate'].values.tolist() if x is not None ])) ))\n",
    "            \n",
    "        if reruninasc:\n",
    "            sleep(3)\n",
    "            x = requests.get(searchterm+\"?page=1&pageSize=100&bibjson.title&sort=bibjson.year:asc\")\n",
    "            if x.status_code == 200:\n",
    "                jsonresult =  x.json()\n",
    "                journaldf = itterate_over_results(searchterm, jsonresult, journals, journaldf, desc=False)\n",
    "                \n",
    "        # itterate over untill we get new Article\n",
    "        #  not working fully if there is a year when mpre than 2000 article was publishd\n",
    "        if middlepart:\n",
    "            # itterate over the years\n",
    "            while True:\n",
    "                yearsearchtext = searchtext + ' AND bibjson.year: \"'+str(middlestart)+'\"'\n",
    "                searchterm = urllib.parse.quote(yearsearchtext)\n",
    "                searchterm = 'https://doaj.org/api/v2/search/articles/'+searchterm\n",
    "                x = requests.get(searchterm+'?page=1&pageSize=100&bibjson.title&sort=bibjson.year:asc')\n",
    "                if x.status_code == 200:\n",
    "                    jsonresult =  x.json()\n",
    "                    if 'total' in jsonresult.keys() :\n",
    "                        if jsonresult['total'] == 0:\n",
    "                            break\n",
    "                        if jsonresult['total'] > 1000 :\n",
    "                            reruninasc = True\n",
    "                        if jsonresult['total'] > 2000 :\n",
    "                            print(\"Can not download all article:\",\n",
    "                                  searchterm+'?page=1&pageSize=100&bibjson.title&sort=bibjson.year:asc' )\n",
    "                    else:\n",
    "                        break\n",
    "                    journaldf = itterate_over_results(searchterm+'AND bibjson.year: \"'+str(middlestart)+'\"', jsonresult, journals, journaldf, desc=False)\n",
    "                middlestart = middlestart-1   \n",
    "                \n",
    "                if reruninasc:\n",
    "                    sleep(3)\n",
    "                    x = requests.get(searchterm+\"?page=1&pageSize=100&bibjson.title&sort=bibjson.year:asc\")\n",
    "                    if x.status_code == 200:\n",
    "                        jsonresult =  x.json()\n",
    "                        journaldf = itterate_over_results(searchterm, jsonresult, journals, journaldf, desc=False)                    \n",
    "\n",
    "            \n",
    "    # drop duplicates\n",
    "    journaldf.drop_duplicates(inplace=True)\n",
    "    \n",
    "    if len(journaldf) > 0:\n",
    "        # save journaldf\n",
    "        journaldf.to_pickle(os.path.join(\n",
    "            datadir,\n",
    "            'journal_'+journals.iloc[idx]['title'].replace(\" \", \"_\").replace(\"&\", \"and\")+\"_\"+version+\".pandas\" )\n",
    "                       )\n",
    "    print(\"Downloaded\", len(journaldf), \"article for \", journals.iloc[idx]['title'] ,\"!\")\n",
    "    sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
