{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = pd.read_csv(\"doaj_journals.csv\", sep=\"|\")\n",
    "journals = journals[journals['publisher'] == \"Optical Society of America (OSA)\"]\n",
    "journals = journals.to_dict('records')\n",
    "print(journals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(journal, filename ):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    driver.get(journal['address'])\n",
    "    print(driver.current_url)\n",
    "    sleep(5)\n",
    "    \n",
    "    # itterate over the journal searching to get the Article list\n",
    "    articles = []\n",
    "    def pagefind():\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # found the Article\n",
    "        for h5 in soup.find_all(\"h3\"):\n",
    "            # get link \n",
    "            for a in h5.find_all(\"a\"):        \n",
    "                articles.append(\"https://www.osapublishing.org\"+a['href'])\n",
    "    \n",
    "        # find next page\n",
    "        for ulp in driver.find_elements_by_class_name('pagination'):\n",
    "            try:\n",
    "                for lis in ulp.find_elements_by_css_selector('li.active + li a'):\n",
    "                    print(lis.text)\n",
    "                    lis.click()\n",
    "                    sleep(random.uniform(15, 45))                        \n",
    "                    return True\n",
    "            except exception as e:\n",
    "                print(e)\n",
    "                return False\n",
    "        return False\n",
    "\n",
    "    nextpage = True\n",
    "    while  nextpage:\n",
    "        nextpage = pagefind()                \n",
    "        print(driver.current_url, end='\\r')\n",
    "        \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articles = list( set(articles).difference(set(oldarticles) ))\n",
    "    print(len(articles), \"new article found!\")\n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': list(set(articles)), \n",
    "                       'journal_title': journal['title'], \n",
    "                       'journal_eissn': journal['eissn'],\n",
    "                       'journal_pissn': journal['pissn'],\n",
    "                       'category': journal['categories']\n",
    "                      })\n",
    "    # chek there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticledetails(df, olddf, filename):\n",
    "    \n",
    "    articles = df['url'].values\n",
    "\n",
    "    titles = [ None for _ in range(len(df))]\n",
    "    abstracts = [ None for _ in range(len(df))]\n",
    "    writers = [ None for _ in range(len(df))]\n",
    "    dates = [ None for _ in range(len(df))]\n",
    "    dois = [  None for _ in range(len(df)) ]\n",
    "    keywords = [  None for _ in range(len(df)) ]\n",
    "\n",
    "    for idx in range(len(articles)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(df),2))+\"%\" , end='\\r')\n",
    "        \n",
    "        url = df.iloc[idx]['url']\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header('Accept-Encoding', 'utf-8')\n",
    "        try:\n",
    "            response = urlopen(request)    \n",
    "            page_content = response.read().decode('utf-8')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        page_soup = BeautifulSoup(page_content, features=\"lxml\")\n",
    "        \n",
    "        # abstract\n",
    "        abstract = \"\"\n",
    "        abst = page_soup.find(\"h2\", {'id':['Abstract']})\n",
    "        count = 0\n",
    "        while True:\n",
    "            abst = abst.findNext('p')\n",
    "            if abst.get_text() is not None and len(abst.get_text()) > 10:\n",
    "                abstract = abst.get_text()\n",
    "                break\n",
    "            \n",
    "            if count == 3:\n",
    "                break\n",
    "            count = count+1\n",
    "        if len(abstract) > 0:\n",
    "            abstracts[idx] = abstract\n",
    "        else:\n",
    "            # if no abstract we ignore the page\n",
    "            print('No abstract:', url)\n",
    "            continue\n",
    "\n",
    "        # title\n",
    "        titles[idx] = page_soup.find('title').get_text().split(\"|\")[1] \n",
    "        # if all title is capital we change it just the first letter\n",
    "        fullcapital = True\n",
    "        titletext = re.sub(r'[^a-zA-Z]', '', titles[idx], flags=re.UNICODE)\n",
    "        for l in titletext :\n",
    "            if l.isupper() is False:\n",
    "                fullcapital = False\n",
    "                break\n",
    "        if fullcapital:\n",
    "            titles[idx] = titles[idx].lower().title()\n",
    "            \n",
    "        # writer and publish date\n",
    "        writter = []\n",
    "        key = []\n",
    "        for m in page_soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == \"citation_author\":\n",
    "                    writter.append( m['content'].title() )\n",
    "                # add the author institute to the author\n",
    "                elif m['name'] == \"citation_author_institution\":\n",
    "                    writter[-1] = writter[-1] + \"--\" + m['content']\n",
    "                # add writer orcid \n",
    "                elif m['name'] == \"citation_author_orcid\":\n",
    "                    writter[-1] = writter[-1] + \"---\" + m['content']\n",
    "                elif m['name'] == \"dc.date\":\n",
    "                    # change to ISO dateformat\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "                elif m['name'] == \"citation_doi\":\n",
    "                    dois[idx] = m['content']\n",
    "                elif m['name'] == \"dc.subject\":\n",
    "                    key.append( m['content'] )\n",
    "        # writers            \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)        \n",
    "        # keywords\n",
    "        if len(key) > 0:\n",
    "            keywords[idx] =  \"#\".join(key)\n",
    "        \n",
    "        sleep(random.uniform(15, 45))\n",
    "    \n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # merge owith old df\n",
    "    df = pd.concat([df, olddf])\n",
    "    print(df.head())\n",
    "    \n",
    "    # save data\n",
    "    df.to_pickle(os.path.join(datadir, filename))\n",
    "    \n",
    "    # test\n",
    "    test = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    print(test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journalsenrichment = [   \n",
    "    {'eissn': '2334-2536', 'address': 'https://www.osapublishing.org/search.cfm?q=&j=optica&v=&i=&p=&cj=1&cc=0&cr=0'},\n",
    "    {'eissn': '2159-3930', 'address': 'https://www.osapublishing.org/search.cfm?q=&j=ome&v=&i=&p=&cj=1&cc=0&cr=0'},\n",
    "    {'eissn': '1094-4087', 'address': 'https://www.osapublishing.org/search.cfm?q=&j=oe&v=&i=&p=&cj=1&cc=0&cr=0'},\n",
    "    {'eissn': '2578-7519', 'address': 'https://www.osapublishing.org/search.cfm?q=&j=osac&v=&i=&p=&cj=1&cc=0&cr=0'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jidx in journals:\n",
    "    for eidx in journalsenrichment:\n",
    "        if jidx['eissn'] == eidx['eissn']:\n",
    "            jidx['address'] = eidx['address']\n",
    "            break\n",
    "print(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(journals)\n",
    "for jidx in range(len(journals)):\n",
    "    filename = 'journal_'+ journals[jidx]['title'].replace(\" \", \"_\").replace(\":\", \"\") +'_'+version+'.pandas'\n",
    "    \n",
    "    # search for articles\n",
    "    df, olddf = gedjournalarticles(journals[jidx], filename )\n",
    "    \n",
    "    # get the articles details\n",
    "    getarticledetails(df, olddf, filename)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[16]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
