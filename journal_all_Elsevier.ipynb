{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.set_window_size(1920, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(journal, filename ):\n",
    "    start_url = \"https://www.sciencedirect.com/search\"\n",
    "    driver.get(start_url)\n",
    "    print(driver.current_url)\n",
    "    sleep(4)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    \n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [], 'journal': [] })\n",
    "    try:\n",
    "        olddf = pd.read_csv(filename, sep=\"|\")\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # set journal\n",
    "    # publisher = driver.find_element_by_id(\"docId\")\n",
    "    # publisher.send_keys(journal['eissn'])\n",
    "    publisher = driver.find_element_by_id(\"pub\")\n",
    "    publisher.send_keys(journal['title'])\n",
    "    # just for Research articles\n",
    "    try:\n",
    "        driver.find_element_by_id(\"adv-search-FLA\").click()\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    sleep(4)\n",
    "    # send request\n",
    "    found = False\n",
    "    for sl in driver.find_elements_by_class_name('search-row'):\n",
    "        for search in sl.find_elements_by_tag_name('button'):\n",
    "            if search.text == \"Search\":\n",
    "                search.click()\n",
    "                sleep(5)\n",
    "                found = True\n",
    "        if found:\n",
    "            break    \n",
    "    print(driver.current_url)\n",
    "    \n",
    "    articles = []\n",
    "    def pagefind():\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # found the Article\n",
    "        for adiv in soup.find_all('div', {'class': ['result-item-container']}):\n",
    "            # check it is open access research article \n",
    "            divtype = adiv.find(\"div\", {'class': ['OpenAccessArchive']}).get_text()\n",
    "            if re.search('Open access', divtype ) and re.search('Research article', divtype ):\n",
    "                for h2 in adiv.find_all(\"h2\"):\n",
    "                    # get link \n",
    "                    for a in h2.find_all(\"a\"):\n",
    "                        # filter out \"Editorial Board\"\n",
    "                        if a.get_text() != \"Editorial Board\":\n",
    "                            articles.append(\"https://www.sciencedirect.com\"+a['href'])\n",
    "    \n",
    "        # try to find modal and close it\n",
    "        for modal in driver.find_elements_by_class_name('modal-close-button'):\n",
    "            modal.click()\n",
    "            sleep(2)\n",
    "        \n",
    "        # avoid feedback bottom\n",
    "        driver.set_window_size(1920, 1080)\n",
    "    \n",
    "        for pl in driver.find_elements_by_class_name('pagination-link'):        \n",
    "            for a in pl.find_elements_by_tag_name('a'):\n",
    "                if a.text == \"next\":\n",
    "                    a.click()            \n",
    "                    sleep(2)\n",
    "                    return True\n",
    "\n",
    "    # itterate over search pages on the Next bottom\n",
    "    nextpage = True\n",
    "    offset = 0\n",
    "    while  nextpage:\n",
    "        nextpage = pagefind()\n",
    "        print(driver.current_url, end='\\r')\n",
    "        \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articles = list( set(articles).difference(set(oldarticles) ))\n",
    "    print(len(articles), \"new article found!\")    \n",
    "    \n",
    "    df = pd.DataFrame({'url': articles, 'journal_title': journal['title'], \n",
    "                       'journal_eissn': journal['eissn'],\n",
    "                       'category': journal['category'] })\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getarticledetails(df, olddf, filename):\n",
    "\n",
    "    articles = df['url'].values\n",
    "    \n",
    "    titles = [ None for _ in range(len(df))]\n",
    "    abstracts = [ None for _ in range(len(df))]\n",
    "    writers = [ None for _ in range(len(df))]\n",
    "    dates = [ None for _ in range(len(df))]\n",
    "    dois = [  None for _ in range(len(df)) ]\n",
    "    keywords = [  None for _ in range(len(df)) ]\n",
    "\n",
    "    for idx in range(len(articles)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(df),2))+\"%\" , end='\\r')\n",
    "    \n",
    "        url = df.iloc[idx]['url']\n",
    "        driver.get(url)\n",
    "        sleep(5)\n",
    "        content = driver.page_source\n",
    "    \n",
    "        page_soup = BeautifulSoup(content, features=\"lxml\", from_encoding='utf-8') \n",
    "    \n",
    "        # abstract\n",
    "        try:\n",
    "            abstractdiv = page_soup.find(lambda tag: tag.name == 'div' and tag['class'] == ['abstract', 'author', ''])\n",
    "            abstracts[idx] = abstractdiv.find(\"p\").text\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", url)\n",
    "            print(e)\n",
    "            break\n",
    "            continue\n",
    "\n",
    "        # title, doi and publication date\n",
    "        for m in page_soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == \"citation_title\":\n",
    "                    titles[idx] = m['content']            \n",
    "                elif m['name'] == \"citation_online_date\":\n",
    "                    # change to ISO dateformat\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                elif m['name'] == \"citation_doi\":\n",
    "                    dois[idx] = m['content']    \n",
    "\n",
    "        # keywords\n",
    "        key = []\n",
    "        try:\n",
    "            for kdiv in page_soup.find(\"div\", {'class':['keywords-section']}).find_all(\"div\", {'class':['keyword']}):\n",
    "                key.append(kdiv.get_text())\n",
    "        except:\n",
    "            pass\n",
    "        if len(key) > 0:\n",
    "            keywords[idx] =  \"#\".join(key)\n",
    "                    \n",
    "        #  language\n",
    "        try:\n",
    "            for script in page_soup.find_all(\"script\"):\n",
    "                if re.search(\"abstracts\", str(script.string)):\n",
    "                    data = json.loads(script.string)\n",
    "                    language = data['abstracts']['content'][0]['$']['lang']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # writers\n",
    "        writter = []\n",
    "        aut = driver.find_element_by_id('author-group')\n",
    "        for auta in aut.find_elements_by_class_name(\"workspace-trigger\"):\n",
    "            auta.click()\n",
    "            sleep(1)\n",
    "            # refresh page\n",
    "            content = driver.page_source\n",
    "            page_soup = BeautifulSoup(content, features=\"lxml\", from_encoding='utf-8')\n",
    "            auta.click()\n",
    "            # refresh page\n",
    "            content = driver.page_source\n",
    "            page_soup = BeautifulSoup(content, features=\"lxml\", from_encoding='utf-8')            \n",
    "            for autdiv in page_soup.find_all(\"div\", {'class':['WorkspaceAuthor']}):\n",
    "                givenname = \"\"\n",
    "                for aut_span in autdiv.find_all(\"span\", {'class':['given-name']}):\n",
    "                    givenname = aut_span.text\n",
    "                    break\n",
    "                familyname = \"\"\n",
    "                for aut_span in autdiv.find_all(\"span\", {'class':['surname']}):\n",
    "                    familyname = aut_span.text\n",
    "                    break\n",
    "                institude = \"\"\n",
    "                for aut_span in autdiv.find_all(\"div\", {'class':['affiliation']}):\n",
    "                    institude = aut_span.text\n",
    "                    break\n",
    "                writter.append(familyname+\", \"+givenname + \"--\"+institude)\n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        \n",
    "\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # merge with old df\n",
    "    df = pd.concat([df, olddf])\n",
    "    \n",
    "    # save data\n",
    "    df.to_csv(filename, sep=\"|\", index=False)\n",
    "                \n",
    "    test = pd.read_csv(filename, sep=\"|\")\n",
    "    print(test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = [ \n",
    "    {'title':\"Climate Risk Management\", 'eissn': '2212-0963', 'category': 'Earth Science' },\n",
    "    { 'title': 'Materials & Design', 'eissn': '1873-4197', 'category': 'Industrial'},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = [    \n",
    "    { 'title': 'Engineering Science and Technology, an International Journal', 'eissn': '2215-0986', 'category': 'Civil Engineering'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sciencedirect.com/search\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: Unable to locate element: [id=\"pub\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c23a83b21c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/journal_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"&\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"and\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0molddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgedjournalarticles\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mjournals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgetarticledetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0molddf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3fc185d8f8dd>\u001b[0m in \u001b[0;36mgedjournalarticles\u001b[0;34m(journal, filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# publisher = driver.find_element_by_id(\"docId\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# publisher.send_keys(journal['eissn'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpublisher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpublisher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjournal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# just for Research articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_id\u001b[0;34m(self, id_)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \"\"\"\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[name=\"%s\"]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             'value': value})['value']\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: Unable to locate element: [id=\"pub\"]\n"
     ]
    }
   ],
   "source": [
    "jidx = 0\n",
    "filename = filename = 'data/journal_'+ journals[jidx]['title'].replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\") +'_'+version+'.csv'\n",
    "df, olddf = gedjournalarticles( journals[jidx], filename )\n",
    "getarticledetails(df, olddf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jidx in range(len(journals)):\n",
    "    filename = filename = 'data/journal_'+ journals[jidx]['title'].replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\") +'_'+version+'.csv'\n",
    "    \n",
    "    # search for articles\n",
    "    df, olddf = gedjournalarticles( journals[jidx], filename )\n",
    "    \n",
    "    # get the articles details\n",
    "    getarticledetails(df, olddf, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
