{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article collection from http://www.geologica-acta.com. They aggraid with data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    driver.get(\"https://revistes.ub.edu/index.php/GEOACTA/search/titles\")\n",
    "    \n",
    "    # itterate over the journal searching to get the Article list\n",
    "    articles = []\n",
    "    def pagefind():\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # found the Article\n",
    "        for tr in soup.find_all(\"tr\"):\n",
    "            # if the row is the ones which have the information\n",
    "            if tr.has_attr(\"valign\"):\n",
    "                if tr['valign'] == \"top\":\n",
    "                    # get the 3td whch is the link\n",
    "                    td = tr.find_all(\"td\")[2]\n",
    "                    # here the first on is the link to the article\n",
    "                    for a in td.find_all('a'):\n",
    "                        articles.append(a[\"href\"])\n",
    "                        break\n",
    "        # find next\n",
    "        # iterate over links and find \">\"\n",
    "        for a in driver.find_elements_by_tag_name('a'): \n",
    "            if a.text.strip() == \">\":\n",
    "                try:\n",
    "                    a.click()\n",
    "                    sleep(2)\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(\"Error to go next page\", str(e))\n",
    "                    return False\n",
    "        return False    \n",
    "\n",
    "    nextpage = True\n",
    "    while  nextpage:\n",
    "        nextpage = pagefind()                \n",
    "        print(driver.current_url, end='\\r')\n",
    "        \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articles = list( set(articles).difference(set(oldarticles) ))\n",
    "    print(len(articles), \"new article found!\")\n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': list(set(articles)), \n",
    "                       'journal_title': \"Geologica Acta\", \n",
    "                       'journal_eissn': \"1696-5728\",\n",
    "                       'journal_pissn': '1695-6133',\n",
    "                       'category': \"Geology\"\n",
    "                      })\n",
    "    # chek there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process new Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticledetails(df, olddf, filename):\n",
    "    \n",
    "    articles = df['url'].values\n",
    "\n",
    "    titles = [ None for _ in range(len(df))]\n",
    "    abstracts = [ None for _ in range(len(df))]\n",
    "    writers = [ None for _ in range(len(df))]\n",
    "    dates = [ None for _ in range(len(df))]\n",
    "    dois = [  None for _ in range(len(df)) ]\n",
    "    keywords = [  None for _ in range(len(df)) ]\n",
    "\n",
    "    for idx in range(len(articles)):\n",
    "        sleep(3)\n",
    "        \n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(df),2))+\"%\" , end='\\r')\n",
    "    \n",
    "        url = df.iloc[idx]['url']\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header('Accept-Encoding', 'utf-8')\n",
    "        try:\n",
    "            response = urlopen(request)    \n",
    "            page_content = response.read().decode('utf-8')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        page_soup = BeautifulSoup(page_content, features=\"lxml\")\n",
    "\n",
    "        # keywords, writers and dates\n",
    "        writter = []\n",
    "        for m in page_soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == 'DC.Type.articleType':\n",
    "                    if  m['content'].strip() != 'Articles':\n",
    "                        print(\"Not a Article\", url)\n",
    "                        continue\n",
    "                if m['name'] == \"keywords\":\n",
    "                    keywords[idx] =  m['content'].replace(\";\", \"#\")\n",
    "                if m['name'] == \"DC.Date.issued\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'DC.Creator.PersonalName':\n",
    "                    writter.append(m['content'])\n",
    "                if m['name'] == 'DC.Identifier.DOI' or m['content'] == 'citation_doi':\n",
    "                    dois[idx] = m['content']                    \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        \n",
    "        # abstract\n",
    "        abstract = \"\"\n",
    "        try:\n",
    "            abstdiv = page_soup.find(\"div\", {\"id\": \"articleAbstract\"}) \n",
    "            # find the firs div as there is the text\n",
    "            abstract = abstdiv.find('div').get_text().replace(\"\\xa0\", \" \")              \n",
    "        except Exception as e:\n",
    "            print(\"No abstract for \", url, str(e))\n",
    "            continue\n",
    "        if len(abstract) > 0:\n",
    "            abstracts[idx] = abstract\n",
    "        else:\n",
    "            print(\"No abstract for \", url)\n",
    "            # if no abstract we ignore the page\n",
    "            continue\n",
    "        \n",
    "        # title\n",
    "        titles[idx] = page_soup.find('title').get_text().split(\"|\")[0] \n",
    "        # if all title is capital we change it just the first letter\n",
    "        fullcapital = True\n",
    "        titletext = re.sub(r'[^a-zA-Z]', '', titles[idx], flags=re.UNICODE)\n",
    "        for l in titletext :\n",
    "            if l.isupper() is False:\n",
    "                fullcapital = False\n",
    "                break\n",
    "        if fullcapital:\n",
    "            titles[idx] = titles[idx].lower().title()                    \n",
    "        \n",
    "                    \n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # merge owith old df\n",
    "    df = pd.concat([df, olddf])\n",
    "    \n",
    "    # save data\n",
    "    df.to_pickle(os.path.join(datadir, filename))\n",
    "    \n",
    "    # test\n",
    "    test = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    print(test.head())                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   url   journal_title  \\\n",
      "500  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "501  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "502  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "503  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "504  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "\n",
      "    journal_eissn journal_pissn category  \\\n",
      "500     1696-5728     1695-6133  Geology   \n",
      "501     1696-5728     1695-6133  Geology   \n",
      "502     1696-5728     1695-6133  Geology   \n",
      "503     1696-5728     1695-6133  Geology   \n",
      "504     1696-5728     1695-6133  Geology   \n",
      "\n",
      "                                                 title  \\\n",
      "500  Traces within traces: holes, pits and gallerie...   \n",
      "501  Influence of latitude and climate on spread, r...   \n",
      "502  Palynological age constraint of Les Vilelles u...   \n",
      "503  The Cambrian-Ordovician siliciclastic platform...   \n",
      "504  Palaeontology of the upper Turonian paralic de...   \n",
      "\n",
      "                                  doi  \\\n",
      "500             10.1344/105.000001620   \n",
      "501             10.1344/105.000001701   \n",
      "502  10.1344/GeologicaActa2015.13.4.7   \n",
      "503             10.1344/105.000001591   \n",
      "504  10.1344/GeologicaActa2016.14.1.5   \n",
      "\n",
      "                                              abstract  \\\n",
      "500  Fossil insect nests with constructed walls (ic...   \n",
      "501  Our aim is to evaluate the influence of climat...   \n",
      "502  Les Vilelles unit is a detrital sequence expos...   \n",
      "503  The Lower Palaeozoic sedimentary cover of the ...   \n",
      "504  The upper Turonian lignite deposits of Sainte-...   \n",
      "\n",
      "                                                writer publishdate  \\\n",
      "500                Radek Mikulás#Jorge Fernando Genise  2003-01-14   \n",
      "501                               C. COIFFARD#B. GÓMEZ  2012-10-03   \n",
      "502       F. GONZÁLEZ#C. MORENO#J.C. MELGAREJO#R. SÁEZ  2015-12-21   \n",
      "503                        L.A. Spalletti#A. Del Valle  2011-07-05   \n",
      "504  D. NÉRADEAU#S. SAINT MARTIN#D.J. BATTEN#J.-P. ...  2016-03-16   \n",
      "\n",
      "                                               keyword  \n",
      "500  Composite specimens. Holes. Pits. Galleries# I...  \n",
      "501  Hyblean Plateau# GPS# Active tectonics# Deform...  \n",
      "502  Les Vilelles unit# Catalonian Coastal Chains# ...  \n",
      "503  Cambrian-Ordovician# Quartz arenites# Shallow ...  \n",
      "504  Amber# Plants# Palynomorphs# Turonian# Cretace...  \n",
      "1 new article found!edu/index.php/GEOACTA/search/titles?searchPage=11#results\n",
      "                                                 url   journal_title  \\\n",
      "0  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "0  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "1  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "2  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "3  https://revistes.ub.edu/index.php/GEOACTA/arti...  Geologica Acta   \n",
      "\n",
      "  journal_eissn journal_pissn category  \\\n",
      "0     1696-5728     1695-6133  Geology   \n",
      "0     1696-5728     1695-6133  Geology   \n",
      "1     1696-5728     1695-6133  Geology   \n",
      "2     1696-5728     1695-6133  Geology   \n",
      "3     1696-5728     1695-6133  Geology   \n",
      "\n",
      "                                               title  \\\n",
      "0  First report of Devonian corals from the Bitli...   \n",
      "0                                               None   \n",
      "1  The demosponge Leptomitus cf. L. lineatus, fir...   \n",
      "2  Subduction consequences along the Andean margi...   \n",
      "3  Conductive structures around Las Cañadas calde...   \n",
      "\n",
      "                                doi  \\\n",
      "0    10.1344/GeologicaActa2021.19.1   \n",
      "0             10.1344/105.000001416   \n",
      "1             10.1344/105.000001597   \n",
      "2  10.1344/GeologicaActa2014.12.4.2   \n",
      "3             10.1344/105.000001516   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  The Bitlis-Pötürge Massif of SE Turkey is a me...   \n",
      "0                                               None   \n",
      "1  The Middle Cambrian Murero Formation in the Ib...   \n",
      "2  All along the eastern border of the Andes lie ...   \n",
      "3  External eastern areas of the Las Cañadas cald...   \n",
      "\n",
      "                                    writer publishdate  \\\n",
      "0              Julien Denayer#Izzet Hoşgör  2021-01-28   \n",
      "0                   Guillermo F. Aceñolaza  2005-01-11   \n",
      "1           Diego García-Bellido Capdevila  2011-07-05   \n",
      "2                    R. BAUDINO#W. HERMOZA  2014-12-09   \n",
      "3  N.P. COPPO#P. SCHNEGG#P. FALCO#R. COSTA  2010-01-01   \n",
      "\n",
      "                                             keyword  \n",
      "0  Arabian Plate# Frasnian# Frechastraea# Rugosa#...  \n",
      "0                                               None  \n",
      "1  Porifera# Burgess Shale-type biota# Commensali...  \n",
      "2  Andes# Marañon Basin# Neogene# Subduction# Oce...  \n",
      "3  Magnetotelluric method# Caldera# Hydrothermal ...  \n"
     ]
    }
   ],
   "source": [
    "filename = 'journal_Geologica_Acta_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "    \n",
    "# get the articles details\n",
    "getarticledetails(df, olddf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
