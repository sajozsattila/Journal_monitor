{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changelog\n",
    " + 1.0.1 -- Add Journal eISSN, and Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(journal, filename ):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [], 'journal': [] })\n",
    "    try:\n",
    "        olddf = pd.read_csv(filename, sep=\"|\")\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    driver.get(journal['address'])\n",
    "    print(driver.current_url)\n",
    "    \n",
    "    # itterate over the journal searching to get the Article list\n",
    "    articles = []\n",
    "    def pagefind():\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # found the Article\n",
    "        for h5 in soup.find_all(\"h5\"):\n",
    "            # get link \n",
    "            for a in h5.find_all(\"a\"):        \n",
    "                articles.append(\"https://www.cambridge.org\"+a['href'])\n",
    "    \n",
    "        # find next page\n",
    "        for ulp in driver.find_elements_by_class_name('pagination'):    \n",
    "            for lis in ulp.find_elements_by_tag_name('a'):\n",
    "                if re.search('Next', lis.text):\n",
    "                    # go to the next page\n",
    "                    lis.click()\n",
    "                    sleep(2)\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    nextpage = True\n",
    "    while  nextpage:\n",
    "        nextpage = pagefind()                \n",
    "        print(driver.current_url, end='\\r')\n",
    "        \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articles = list( set(articles).difference(set(oldarticles) ))\n",
    "    print(len(articles), \"new article found!\")\n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': list(set(articles)), \n",
    "                       'journal_title': journal['title'], \n",
    "                       'journal_eissn': journal['eissn'],\n",
    "                       'category': journal['category']\n",
    "                      })\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process new articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticledetails(df, olddf, filename):\n",
    "    \n",
    "    articles = df['url'].values\n",
    "\n",
    "    titles = [ None for _ in range(len(df))]\n",
    "    abstracts = [ None for _ in range(len(df))]\n",
    "    writers = [ None for _ in range(len(df))]\n",
    "    dates = [ None for _ in range(len(df))]\n",
    "    dois = [  None for _ in range(len(df)) ]\n",
    "    keywords = [  None for _ in range(len(df)) ]\n",
    "\n",
    "    for idx in range(len(articles)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(df),2))+\"%\" , end='\\r')\n",
    "    \n",
    "        url = df.iloc[idx]['url']\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header('Accept-Encoding', 'utf-8')\n",
    "        try:\n",
    "            response = urlopen(request)    \n",
    "            page_content = response.read().decode('utf-8')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        page_soup = BeautifulSoup(page_content, features=\"lxml\", from_encoding='utf-8')\n",
    "        \n",
    "        # abstract\n",
    "        abstract = \"\"\n",
    "        for abstdiv in page_soup.findAll(\"div\", {'class':['abstract']}):\n",
    "            for ap in abstdiv.find_all(\"p\"):\n",
    "                abstract = abstract + (\" \".join(ap.get_text().splitlines()).replace(\"\\xa0\", \" \") )\n",
    "        if len(abstract) > 0:\n",
    "            abstracts[idx] = abstract\n",
    "        else:\n",
    "            # if no abstract we ignore the page\n",
    "            continue\n",
    "\n",
    "        # title\n",
    "        for t in page_soup.find_all('title'):        \n",
    "            titles[idx] = t.get_text().split(\"|\")[0]                \n",
    "            \n",
    "        # writer and publish date\n",
    "        writter = []\n",
    "        for m in page_soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == \"citation_author\":\n",
    "                    writter.append( m['content'] )\n",
    "                # add the author institute to the author\n",
    "                elif m['name'] == \"citation_author_institution\":\n",
    "                    writter[-1] = writter[-1] + \"--\" + m['content']\n",
    "                # add writer orcid \n",
    "                elif m['name'] == \"citation_author_orcid\":\n",
    "                    writter[-1] = writter[-1] + \"---\" + m['content']\n",
    "                elif m['name'] == \"citation_online_date\":\n",
    "                    # change to ISO dateformat\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                elif m['name'] == \"citation_doi\":\n",
    "                    dois[idx] = m['content']\n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        \n",
    "        # keywords\n",
    "        key = []\n",
    "        for keya in page_soup.findAll(\"a\", {'class':['keywords']}):\n",
    "            key.append(keya.text)\n",
    "        if len(key) > 0:\n",
    "            keywords[idx] =  \"#\".join(key)\n",
    "    \n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # merge owith old df\n",
    "    df = pd.concat([df, olddf])\n",
    "    \n",
    "    # save data\n",
    "    df.to_csv(filename, sep=\"|\", index=False)\n",
    "    \n",
    "    # test\n",
    "    test = pd.read_csv(filename, sep=\"|\")\n",
    "    print(test.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = [\n",
    "    { \n",
    "        'title': 'Annals_of_Glaciology', \n",
    "        'eissn': '1727-5644', \n",
    "        'category': 'Earth Science', \n",
    "        'address': 'https://www.cambridge.org/core/journals/annals-of-glaciology/listing?q=&searchWithinIds=DD7BD3FCBF82BF09609E5A0E1888ED7B&aggs[productTypes][filters]=JOURNAL_ARTICLE'\n",
    "    }, \n",
    "    { \n",
    "        'title': 'Journal_of_Glaciology',\n",
    "        'eissn': '1727-5652', \n",
    "        'category': 'Earth Science', \n",
    "        'address': 'https://www.cambridge.org/core/journals/journal-of-glaciology/listing?q=&searchWithinIds=FE8284B2577ADBCD299821FDE4E752F1&aggs[productTypes][filters]=JOURNAL_ARTICLE'\n",
    "    } ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cambridge.org/core/journals/annals-of-glaciology/listing?q=&searchWithinIds=DD7BD3FCBF82BF09609E5A0E1888ED7B&aggs[productTypes][filters]=JOURNAL_ARTICLE\n",
      "3855 new article found!rg/core/journals/annals-of-glaciology/listing?aggs%5BproductTypes%5D%5Bfilters%5D=JOURNAL_ARTICLE&pageNum=193&searchWithinIds=DD7BD3FCBF82BF09609E5A0E1888ED7B\n",
      "0.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:220: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    url         journal_title  \\\n",
      "3850  https://www.cambridge.org/core/journals/annals...  Annals_of_Glaciology   \n",
      "3851  https://www.cambridge.org/core/journals/annals...  Annals_of_Glaciology   \n",
      "3852  https://www.cambridge.org/core/journals/annals...  Annals_of_Glaciology   \n",
      "3853  https://www.cambridge.org/core/journals/annals...  Annals_of_Glaciology   \n",
      "3854  https://www.cambridge.org/core/journals/annals...  Annals_of_Glaciology   \n",
      "\n",
      "     journal_eissn       category  \\\n",
      "3850     1727-5644  Earth Science   \n",
      "3851     1727-5644  Earth Science   \n",
      "3852     1727-5644  Earth Science   \n",
      "3853     1727-5644  Earth Science   \n",
      "3854     1727-5644  Earth Science   \n",
      "\n",
      "                                                  title  \\\n",
      "3850  Assessment of the surface mass balance along t...   \n",
      "3851  The simulation of Antarctic sea ice in the Had...   \n",
      "3852  In situ stress tensor measured in an Alaskan g...   \n",
      "3853  Initial experiments on the effects of particle...   \n",
      "3854  Sea-ice surface roughness estimates from airbo...   \n",
      "\n",
      "                             doi  \\\n",
      "3850  10.3189/172756405781812682   \n",
      "3851  10.3189/172756401781818095   \n",
      "3852  10.3189/172756400781820354   \n",
      "3853  10.3189/172756404781814069   \n",
      "3854       10.3189/2015AoG69A717   \n",
      "\n",
      "                                               abstract  \\\n",
      "3850  This paper explores the potential of using sat...   \n",
      "3851  An assessment is presented of the extent and v...   \n",
      "3852  An experimental program at Worthington Glacier...   \n",
      "3853  Experimental observations of the influence of ...   \n",
      "3854  We present sea-ice surface roughness estimates...   \n",
      "\n",
      "                                                 writer publishdate  \\\n",
      "3850  Wouter Greuell--Institute for Marine and Atmos...  2017-09-14   \n",
      "3851  John Turner--British Antarctic Survey, Natural...  2017-09-14   \n",
      "3852  W. T. Pfeffer--Institute of Arctic and Alpine ...  2017-09-14   \n",
      "3853  Min Song--Thayer School of Engineering, Dartmo...  2017-09-14   \n",
      "3854  Justin F. Beckers--University of Alberta#Angel...  2017-07-26   \n",
      "\n",
      "                                                keyword  journal  \n",
      "3850                                                NaN      NaN  \n",
      "3851                                                NaN      NaN  \n",
      "3852                                                NaN      NaN  \n",
      "3853                                                NaN      NaN  \n",
      "3854  aerogeophysical measurements#airborne electrom...      NaN  \n",
      "https://www.cambridge.org/core/journals/journal-of-glaciology/listing?q=&searchWithinIds=FE8284B2577ADBCD299821FDE4E752F1&aggs[productTypes][filters]=JOURNAL_ARTICLE\n",
      "5502 new article found!rg/core/journals/journal-of-glaciology/listing?aggs%5BproductTypes%5D%5Bfilters%5D=JOURNAL_ARTICLE&pageNum=277&searchWithinIds=FE8284B2577ADBCD299821FDE4E752F1\n",
      "0.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bs4/__init__.py:220: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    url  \\\n",
      "5497  https://www.cambridge.org/core/journals/journa...   \n",
      "5498  https://www.cambridge.org/core/journals/journa...   \n",
      "5499  https://www.cambridge.org/core/journals/journa...   \n",
      "5500  https://www.cambridge.org/core/journals/journa...   \n",
      "5501  https://www.cambridge.org/core/journals/journa...   \n",
      "\n",
      "              journal_title journal_eissn       category  \\\n",
      "5497  Journal_of_Glaciology     1727-5652  Earth Science   \n",
      "5498  Journal_of_Glaciology     1727-5652  Earth Science   \n",
      "5499  Journal_of_Glaciology     1727-5652  Earth Science   \n",
      "5500  Journal_of_Glaciology     1727-5652  Earth Science   \n",
      "5501  Journal_of_Glaciology     1727-5652  Earth Science   \n",
      "\n",
      "                                                  title  \\\n",
      "5497                                                NaN   \n",
      "5498  A 5 year record of surface energy and mass bal...   \n",
      "5499  Borehole imagery of meteoric and marine ice la...   \n",
      "5500  Progress in glacial geology during the last fi...   \n",
      "5501                                                NaN   \n",
      "\n",
      "                             doi  \\\n",
      "5497                         NaN   \n",
      "5498  10.3189/002214308784886199   \n",
      "5499  10.3189/172756505781829511   \n",
      "5500   10.3189/S0022143000215797   \n",
      "5501                         NaN   \n",
      "\n",
      "                                               abstract  \\\n",
      "5497                                                NaN   \n",
      "5498  A 5 year record of data from an automatic weat...   \n",
      "5499  A real-time video camera probe was deployed in...   \n",
      "5500  The principal objectives of glacial geology ov...   \n",
      "5501                                                NaN   \n",
      "\n",
      "                                                 writer publishdate keyword  \\\n",
      "5497                                                NaN         NaN     NaN   \n",
      "5498  Liss M. Andreassen--Section for Glaciers, Snow...  2017-09-08     NaN   \n",
      "5499  Mike Craven--Australian Antarctic Division and...  2017-09-08     NaN   \n",
      "5500  G.S. Boulton--Grant Institute of Geology, Univ...  2018-02-27     NaN   \n",
      "5501                                                NaN         NaN     NaN   \n",
      "\n",
      "      journal  \n",
      "5497      NaN  \n",
      "5498      NaN  \n",
      "5499      NaN  \n",
      "5500      NaN  \n",
      "5501      NaN  \n"
     ]
    }
   ],
   "source": [
    "for jidx in range(len(journals)):\n",
    "    filename = filename = 'data/journal_'+ journals[jidx]['title'] +'_'+version+'.csv'\n",
    "    \n",
    "    # search for articles\n",
    "    df, olddf = gedjournalarticles(journals[jidx], filename )\n",
    "    \n",
    "    # get the articles details\n",
    "    getarticledetails(df, olddf, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
