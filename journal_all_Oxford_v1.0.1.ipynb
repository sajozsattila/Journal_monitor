{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.1\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The Article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.set_window_size(1920, 1080)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(journal, filename ):\n",
    "    start_url = \"https://academic.oup.com/journals/advanced-search\"\n",
    "    driver.get(start_url)\n",
    "    print(driver.current_url)\n",
    "    sleep(4)\n",
    "    \n",
    "    content = driver.page_source\n",
    "    \n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [], 'journal_title': [],\n",
    "                                'title': [], 'writer': [], \n",
    "                                'doi': [], 'abstract': [],\n",
    "                                'keyword': [], 'publishdate': [],\n",
    "                                'language': [],\n",
    "                                'category': [],\n",
    "                                'journal_eissn': [], 'journal_pissn': []})\n",
    "    olddf.set_index('doi', inplace=True)\n",
    "    olddf['doi'] = []\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir,filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df =  pd.DataFrame({'url': [], 'journal_title': [],\n",
    "                                'title': [], 'writer': [], \n",
    "                                'doi': [], 'abstract': [],\n",
    "                                'keyword': [], 'publishdate': [],\n",
    "                                'language': [],\n",
    "                                'category': [],\n",
    "                                'journal_eissn': [], 'journal_pissn': []})\n",
    "    df.set_index('doi', inplace=True)\n",
    "    df['doi'] = []\n",
    "\n",
    "    # set journal\n",
    "    try:\n",
    "        selectpublisher = Select(driver.find_element_by_id(\"SelectedJournal\"))\n",
    "    except Exception as e:\n",
    "        print(\"Error in SelectedJournal: \", e)\n",
    "        return df, olddf\n",
    "    print(journal['title'].split(\":\")[0])\n",
    "    searchtitle = journal['title'].split(\":\")[0]\n",
    "    try:        \n",
    "        # patch for specific articles\n",
    "        if searchtitle == 'AoB Plants':\n",
    "            searchtitle = 'AoB PLANTS'\n",
    "        \n",
    "        selectpublisher.select_by_visible_text(searchtitle)                        \n",
    "    except Exception as e:\n",
    "        print(\"Error in journal selection:\", e)\n",
    "        return df, olddf\n",
    "    a = driver.find_element_by_id(\"btnAdvancedCitationFilter\")\n",
    "    if a is not None:\n",
    "        a.click()\n",
    "    sleep(4)\n",
    "    print(driver.current_url)\n",
    "\n",
    "    # just for Research articles\n",
    "    try:\n",
    "        driver.find_element_by_id(\"article-type-label0\").click()\n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "    \n",
    "    def processresult(df):\n",
    "        sleep(np.random.uniform(3,60))        \n",
    "        \n",
    "    \n",
    "        # open all of the abstract\n",
    "        for selabstract in driver.find_elements_by_class_name('js-show-abstract'):\n",
    "            selabstract.click()\n",
    "            sleep(np.random.uniform(3,120))\n",
    "        \n",
    "        # BeautifulSoup\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get the articles \n",
    "        articles = soup.find_all('div', {'class': ['al-article-box']})\n",
    "        \n",
    "\n",
    "    \n",
    "        for article in articles:\n",
    "            title = article.find(\"h4\").get_text()\n",
    "            url = article.find(\"h4\").find(\"a\")['href']\n",
    "            print(\"url\", url)\n",
    "            date = article.find('div', {'class': ['al-pub-date']}).get_text().replace(\"Published: \", '')\n",
    "            date = datetime.datetime.strptime(date, \"%d %B %Y\").strftime('%Y-%m-%d')        \n",
    "            abstract = article.find('section', {'class': ['abstract']}).get_text()\n",
    "            abstract = re.sub(searchtitle+' URL: .+', '', abstract )\n",
    "            doi = \"\"\n",
    "            doidiv = article.find('div', {'class': ['al-citation-list']}).find_all('a')\n",
    "            for a in doidiv:\n",
    "                if re.search(\"doi.org\", a[\"href\"] ):\n",
    "                    doi = a[\"href\"].replace(\"https://doi.org/\", '')\n",
    "                    break\n",
    "            writers = []\n",
    "            writersdeiv =  article.find('div', {'class': ['al-authors-list']}).find_all( 'span', {'class': ['wi-fullname']} )\n",
    "            for w in writersdeiv:\n",
    "                writers.append(w.get_text())\n",
    "            writers = \"#\".join(writers)\n",
    "        \n",
    "            # add data to the df\n",
    "            datadict = pd.Series({\n",
    "                        'journal_title': journal['title'],\n",
    "                        'url': url,\n",
    "                        'title': title,\n",
    "                        'writer': writers,\n",
    "                        'doi': doi,\n",
    "                        'abstract': abstract,\n",
    "                        'keyword': \"\",\n",
    "                        'publishdate': date,\n",
    "                        'language': \"en\",\n",
    "                        'journal_eissn': journal['eissn'],\n",
    "                        'journal_pissn': journal['pissn'],\n",
    "                        'category': journal['categories']\n",
    "                    }).rename(doi)\n",
    "            df = df.append(datadict) \n",
    "        df.to_pickle(filename)\n",
    "        print(df.tail())\n",
    "        return df\n",
    "    \n",
    "    if not re.search('JournalDisplayName', driver.current_url):\n",
    "        print(\"Error to set Journal:\", driver.current_url)\n",
    "        return df, olddf\n",
    "    \n",
    "    # firs result\n",
    "    df = processresult(df)        \n",
    "    \n",
    "    # iterate over the nexts\n",
    "    while True:\n",
    "        try:\n",
    "            sleep(np.random.uniform(3,60))\n",
    "            for nextbottom in driver.find_elements_by_css_selector(\"div.pagination a.al-nav-next\"):\n",
    "                nextbottom.click()\n",
    "                break\n",
    "            df = processresult(df)\n",
    "        except:\n",
    "            try:\n",
    "                for nextbottom in driver.find_elements_by_css_selector(\"div.al-pageNumbers a.al-nav-next\"):\n",
    "                    nextbottom.click()\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(\"Error on next:\", e)\n",
    "                break\n",
    "                \n",
    "    # chek there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        \n",
    "    return df, olddf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = pd.read_csv(\"doaj_journals.csv\", sep=\"|\")\n",
    "journals = journals[journals['publisher'] == \"Oxford University Press\"]\n",
    "journals = journals.to_dict('records')\n",
    "print(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for jidx in range(len(journals)):\n",
    "    filename = 'journal_'+ journals[jidx]['title'].replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\") +'_'+version+'.pandas'\n",
    "    \n",
    "    # search for articles\n",
    "    df, olddf = gedjournalarticles( journals[jidx], filename )\n",
    "    \n",
    "    # merg olddf and df\n",
    "    df = pd.concat([df,olddf], axis=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
