{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article collection from http://www.geologica-acta.com. They aggraid with data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Article list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    driver.get(\"https://revistes.ub.edu/index.php/GEOACTA/search/titles\")\n",
    "    \n",
    "    # itterate over the journal searching to get the Article list\n",
    "    articles = []\n",
    "    def pagefind():\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # found the Article\n",
    "        for tr in soup.find_all(\"tr\"):\n",
    "            # if the row is the ones which have the information\n",
    "            if tr.has_attr(\"valign\"):\n",
    "                if tr['valign'] == \"top\":\n",
    "                    # get the 3td whch is the link\n",
    "                    td = tr.find_all(\"td\")[2]\n",
    "                    # here the first on is the link to the article\n",
    "                    for a in td.find_all('a'):\n",
    "                        articles.append(a[\"href\"])\n",
    "                        break\n",
    "        # find next\n",
    "        # iterate over links and find \">\"\n",
    "        for a in driver.find_elements_by_tag_name('a'): \n",
    "            if a.text.strip() == \">\":\n",
    "                try:\n",
    "                    a.click()\n",
    "                    sleep(2)\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(\"Error to go next page\", str(e))\n",
    "                    return False\n",
    "        return False    \n",
    "\n",
    "    nextpage = True\n",
    "    while  nextpage:\n",
    "        nextpage = pagefind()                \n",
    "        print(driver.current_url, end='\\r')\n",
    "        \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articles = list( set(articles).difference(set(oldarticles) ))\n",
    "    print(len(articles), \"new article found!\")\n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': list(set(articles)), \n",
    "                       'journal_title': \"Geologica Acta\", \n",
    "                       'journal_eissn': \"1696-5728\",\n",
    "                       'journal_pissn': '1695-6133',\n",
    "                       'category': \"Geology\"\n",
    "                      })\n",
    "    # chek there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process new Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getarticledetails(df, olddf, filename):\n",
    "    \n",
    "    articles = df['url'].values\n",
    "\n",
    "    titles = [ None for _ in range(len(df))]\n",
    "    abstracts = [ None for _ in range(len(df))]\n",
    "    writers = [ None for _ in range(len(df))]\n",
    "    dates = [ None for _ in range(len(df))]\n",
    "    dois = [  None for _ in range(len(df)) ]\n",
    "    keywords = [  None for _ in range(len(df)) ]\n",
    "\n",
    "    for idx in range(len(articles)):\n",
    "        sleep(3)\n",
    "        \n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(df),2))+\"%\" , end='\\r')\n",
    "    \n",
    "        url = df.iloc[idx]['url']\n",
    "        print(url)\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header('Accept-Encoding', 'utf-8')\n",
    "        try:\n",
    "            response = urlopen(request)    \n",
    "            page_content = response.read().decode('utf-8')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        page_soup = BeautifulSoup(page_content, features=\"lxml\")\n",
    "\n",
    "        # keywords, writers and dates\n",
    "        writter = []\n",
    "        for m in page_soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == 'DC.Type.articleType':\n",
    "                    if  m['content'].strip() != 'Articles':\n",
    "                        print(\"Not a Article\")\n",
    "                        continue\n",
    "                if m['name'] == \"keywords\":\n",
    "                    keywords[idx] =  m['content'].replace(\";\", \"#\")\n",
    "                if m['name'] == \"DC.Date.issued\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'DC.Creator.PersonalName':\n",
    "                    writter.append(m['content'])\n",
    "                if m['content'] == 'DC.Identifier.DOI' or m['content'] == 'citation_doi':\n",
    "                    doi[idx] = m['content']                    \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        \n",
    "        # abstract\n",
    "        abstract = \"\"\n",
    "        try:\n",
    "            abstdiv = page_soup.find(\"div\", {\"id\": \"articleAbstract\"}) \n",
    "            # find the firs div as there is the text\n",
    "            abstract = abstdiv.find('div').get_text().replace(\"\\xa0\", \" \")              \n",
    "        except Exception as e:\n",
    "            print(\"No abstract for \", url, str(e))\n",
    "            continue\n",
    "        if len(abstract) > 0:\n",
    "            abstracts[idx] = abstract\n",
    "        else:\n",
    "            print(\"No abstract for \", url)\n",
    "            # if no abstract we ignore the page\n",
    "            continue\n",
    "        \n",
    "        # title\n",
    "        titles[idx] = page_soup.find('title').get_text().split(\"|\")[0] \n",
    "        # if all title is capital we change it just the first letter\n",
    "        fullcapital = True\n",
    "        titletext = re.sub(r'[^a-zA-Z]', '', titles[idx], flags=re.UNICODE)\n",
    "        for l in titletext :\n",
    "            if l.isupper() is False:\n",
    "                fullcapital = False\n",
    "                break\n",
    "        if fullcapital:\n",
    "            titles[idx] = titles[idx].lower().title()                    \n",
    "        \n",
    "                    \n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # merge owith old df\n",
    "    df = pd.concat([df, olddf])\n",
    "    \n",
    "    # save data\n",
    "    df.to_pickle(os.path.join(datadir, filename))\n",
    "    \n",
    "    # test\n",
    "    test = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    print(test.head())                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://revistes.ub.edu/index.php/GEOACTA/search/titles?searchPage=10#results\r"
     ]
    }
   ],
   "source": [
    "filename = 'journal_Geologica Acta_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "    \n",
    "# get the articles details\n",
    "getarticledetails(df, olddf, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
