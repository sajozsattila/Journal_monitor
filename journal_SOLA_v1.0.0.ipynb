{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.0\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.get(\"https://www.jstage.jst.go.jp/browse/sola/list/-char/en\")\n",
    "    \n",
    "    # get all of the issue\n",
    "    def issues():\n",
    "        issues = []\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for a in soup.find_all('a', {'class': 'bluelink-style'}):\n",
    "            # if it is this journal\n",
    "            if re.search(\"Volume\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                issues.append(a['href'])        \n",
    "        \n",
    "        return  issues    \n",
    "    allissue = issues()\n",
    "    allissue = list(set(allissue))\n",
    "    \n",
    "    def articles():\n",
    "        articles = []\n",
    "        nexpage = False\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for a in soup.find_all('a'):\n",
    "            # if it is this journal\n",
    "            if re.search(\"^https://doi.org\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                articles.append(a['href'])\n",
    "        # check there is new issue\n",
    "        for a in driver.find_elements_by_name('a'):\n",
    "            if '>' == a.getText():\n",
    "                nexpage = True\n",
    "                # set to nexpage\n",
    "                a.click()\n",
    "                sleep(3)        \n",
    "        return  articles, nexpage\n",
    "    \n",
    "    # iterate over isses and get article links\n",
    "    articellinks = []\n",
    "    for issue in allissue:\n",
    "        driver.get(issue)\n",
    "        sleep(2)\n",
    "        print(driver.current_url, end='\\r') \n",
    "        # get the issue article\n",
    "        nextpage = True\n",
    "        allissue = []\n",
    "        while nextpage:\n",
    "            thispagearticles, nextpage = articles()\n",
    "            articellinks = articellinks + thispagearticles\n",
    "            print(driver.current_url, end='\\r')        \n",
    "    articellinks = list(set(articellinks))\n",
    "    \n",
    "    # filter out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articellinks = list( set(articellinks).difference(set(oldarticles) ))\n",
    "    print(len(articellinks), \"new article found!\")\n",
    "    \n",
    "    # iterate over article \n",
    "    urls = [ None for _ in range(len(articellinks))]\n",
    "    titles = [ None for _ in range(len(articellinks))]\n",
    "    abstracts = [ None for _ in range(len(articellinks))]\n",
    "    writers = [ None for _ in range(len(articellinks))]\n",
    "    dates = [ None for _ in range(len(articellinks))]\n",
    "    dois = [  None for _ in range(len(articellinks)) ]\n",
    "    keywords = [  None for _ in range(len(articellinks)) ]\n",
    "    for idx in range(len(articellinks)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(articellinks),2))+\"%\" , end='\\r')\n",
    "        url =  articellinks[idx]\n",
    "        # load page\n",
    "        driver.get(articellinks[idx])\n",
    "        sleep(3)        \n",
    "        urls[idx] = driver.current_url\n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # process metadata\n",
    "        keyword = []\n",
    "        writter = []\n",
    "        notpaper = False\n",
    "        for m in soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == 'title':\n",
    "                    titles[idx] = m['content']\n",
    "                if m['name'] == \"citation_keywords\":\n",
    "                    keyword.append( m['content'] )\n",
    "                if m['name'] == \"citation_online_date\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y/%m/%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'citation_author':\n",
    "                    writter.append(m['content'])\n",
    "                if re.search('citation_doi', m['name']):\n",
    "                    dois[idx] = m['content']                    \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        if len(keyword) > 0:\n",
    "            keywords[idx] ==  \"#\".join(keyword)\n",
    "            \n",
    "        # get abstract\n",
    "        abstract = \"\"\n",
    "        if notpaper:\n",
    "            continue\n",
    "        else:\n",
    "            for div in soup.find_all(id='article-overiew-abstract-wrap'):\n",
    "                for c in div.find_all(['p', 'script']):\n",
    "                    if c.name == \"p\":                    \n",
    "                        for tag in  c.contents:\n",
    "                            if re.search(\"<span\", str(tag)):\n",
    "                                continue\n",
    "                            else:\n",
    "                                match = re.match(\"<script.+>\", str(tag))\n",
    "                                if match is not None:\n",
    "                                    abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                                else:\n",
    "                                    abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "                # if still not abstract it is possible it is not in p\n",
    "                if len(abstract) == 0:\n",
    "                    for tag in div.contents:\n",
    "                        if re.search(\"<span\", str(tag)) or re.search(\"<h3\", str(tag)) :\n",
    "                            continue\n",
    "                        else:\n",
    "                            match = re.match(\"<script.+>\", str(tag))\n",
    "                            if match is not None:\n",
    "                                abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                            else:\n",
    "                                abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "            abstracts[idx] = abstract       \n",
    "        \n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': urls, \n",
    "                       'journal_title': \"SOLA\", \n",
    "                       'journal_eissn': \"1349-6476\",\n",
    "                       'journal_pissn': '',\n",
    "                       'category': \"Meteorology. Climatology\",                                              \n",
    "                      })\n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # drop where is no abstract\n",
    "    df = df[ df['abstract'].notna()]\n",
    "    # drop where title is Editorial\n",
    "    df = df[df['title'] != 'Editorial']\n",
    "    # drop where title \"The SOLA Award in *\"\"\n",
    "    df = df[df.title.str.contains('^(?!The SOLA Award in)', regex= True, na=False)]\n",
    "    \n",
    "    \n",
    "    # check there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757 new article found!.go.jp/browse/sola/10/0/_contents/-char/enn\n",
      "                                                 url journal_title  \\\n",
      "0  https://www.jstage.jst.go.jp/article/sola/12/0...          SOLA   \n",
      "1  https://www.jstage.jst.go.jp/article/sola/9/0/...          SOLA   \n",
      "2  https://www.jstage.jst.go.jp/article/sola/8/0/...          SOLA   \n",
      "3  https://www.jstage.jst.go.jp/article/sola/3/0/...          SOLA   \n",
      "4  https://www.jstage.jst.go.jp/article/sola/13/0...          SOLA   \n",
      "\n",
      "  journal_eissn journal_pissn                  category  \\\n",
      "0     1349-6476                Meteorology. Climatology   \n",
      "1     1349-6476                Meteorology. Climatology   \n",
      "2     1349-6476                Meteorology. Climatology   \n",
      "3     1349-6476                Meteorology. Climatology   \n",
      "4     1349-6476                Meteorology. Climatology   \n",
      "\n",
      "                                               title                    doi  \\\n",
      "0  The Structure and Development of an Extratropi...  10.2151/sola.2016-050   \n",
      "1  Parameter Sensitivities of the Dual-Localizati...  10.2151/sola.2013-039   \n",
      "2  Eastward-Propagating Property of Large-Scale P...  10.2151/sola.2012-006   \n",
      "3  Aircraft Triggered Lightning Caused by Winter ...  10.2151/sola.2007-028   \n",
      "4  Why Torrential Rain Occurs on the Western Coas...  10.2151/sola.2017-007   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  The rare snowfall event in 2012 over North Chi...   \n",
      "1  In the ensemble Kalman filter, covariance loca...   \n",
      "2  Using the Non-hydrostatic ICosahedral Atmosphe...   \n",
      "3  A case of aircraft triggered lightning, which ...   \n",
      "4  This study examined the impact of an active ph...   \n",
      "\n",
      "                                              writer publishdate keyword  \n",
      "0  Ying Liu#Donghai Wang#Zhaoming Liang#Chongjian...  2016-09-22    None  \n",
      "1        Keiichi Kondo#Takemasa Miyoshi#H. L. Tanaka  2013-11-27    None  \n",
      "2       Masanori Yoshizaki#Shi-ichi Iga#Masaki Satoh  2012-03-22    None  \n",
      "3  Fumiaki Kobayashi#Takatsugu Shimura#Kazutoshi ...  2007-11-02    None  \n",
      "4  Peiming Wu#Dodi Ardiansyah#Satoru Yokoi#Shuich...  2017-03-24    None  \n"
     ]
    }
   ],
   "source": [
    "filename = 'journal_SOLA_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "\n",
    "# save data\n",
    "df.to_pickle(os.path.join(datadir, filename))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rare snowfall event in 2012 over North China due to the development of an unusual cyclone was analyzed. The results show that the cyclonic vorticity occurs first at the middle troposphere, and then extends to both the upper and lower troposphere. The trigger mechanism for the cyclone genesis is the baroclinic forcing with the upper unusual vorticity advection promoting its rapid development. The efficient deployment of the high- and low-level jets forms a favorable environment for its development and also transports a great number of vapor and the instability energy into the area of blizzard, resulting in the occurrence of the rare event.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url              https://www.jstage.jst.go.jp/article/sola/12/0...\n",
      "journal_title                                                 SOLA\n",
      "journal_eissn                                            1349-6476\n",
      "journal_pissn                                                     \n",
      "category                                  Meteorology. Climatology\n",
      "title            The Structure and Development of an Extratropi...\n",
      "doi                                          10.2151/sola.2016-050\n",
      "abstract         The rare snowfall event in 2012 over North Chi...\n",
      "writer           Ying Liu#Donghai Wang#Zhaoming Liang#Chongjian...\n",
      "publishdate                                             2016-09-22\n",
      "keyword                                                       None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
