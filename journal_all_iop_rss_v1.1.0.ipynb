{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading \n",
    "from queue import Queue\n",
    "import feedparser\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.set_window_size(1920, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jname = \"iop\"\n",
    "version = '1.1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListeningRSS(threading.Thread):\n",
    "    def __init__(self, job):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.stoprequest = threading.Event()\n",
    "        self.mur2job = job\n",
    "        self.memory = {}\n",
    "        self.memoryname = 'data/journal_all_'+jname+'_'+version+'_feeds.pickle'\n",
    "        try:\n",
    "            with open(self.memoryname, 'rb') as handle:\n",
    "                self.memory = pickle.load(handle)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.feeds = pd.read_csv(\"doaj_journals.csv\", sep=\"|\")\n",
    "        self.feeds = self.feeds[self.feeds['publisher'] == \"IOP Publishing\"]\n",
    "        self.feeds = self.feeds.to_dict('records')\n",
    "        \n",
    "    def join(self, timeout=None):\n",
    "        self.stoprequest.set()\n",
    "        super(ListeningRSS, self).join(timeout)\n",
    "    def run(self):\n",
    "        print(self.memory)\n",
    "        while True:\n",
    "            for feedl in self.feeds:\n",
    "                \n",
    "                feedurl = 'https://iopscience.iop.org/journal/rss/'+feedl['eissn']\n",
    "                print(feedurl)\n",
    "                feed = feedparser.parse(feedurl)\n",
    "                # print(feed)\n",
    "                # add feed if it is not in the dictionary\n",
    "                if feedurl not in self.memory.keys():\n",
    "                    self.memory[feedurl] = { 'lasturl': '' }\n",
    "                # check for new feed\n",
    "                newfeedIdx = 0 \n",
    "                for item in feed['items']:\n",
    "                    journal = item['prism_publicationname']\n",
    "                    if journal not in self.memory.keys():\n",
    "                        self.memory[journal] = {}\n",
    "                    \n",
    "                    # get what is possible from the RSS\n",
    "                    url = item['link']\n",
    "                    \n",
    "                    # check url is in the last on or not\n",
    "                    if url not in self.memory[journal].keys():\n",
    "                        self.memory[journal][url] = datetime.datetime.now() \n",
    "                        # keep memory under control, so drop last observation\n",
    "                        if len(self.memory[journal].keys()) > 200:\n",
    "                            # drop oldest\n",
    "                            keys = dict(sorted((value, key) for (key,value) in self.memory[journal].items()))\n",
    "                            key = keys[list(keys.keys())[0]]\n",
    "                            self.memory[journal].pop( key )\n",
    "                        # pickle the memory\n",
    "                        with open(self.memoryname, 'wb') as handle:\n",
    "                            pickle.dump(self.memory, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    else:\n",
    "                        continue                            \n",
    "                    \n",
    "                    title = item['title']\n",
    "                    doi = item['prism_doi']\n",
    "                    \n",
    "                    \n",
    "                    abstract = item['description']\n",
    "                    date = None\n",
    "                    try:\n",
    "                        date = datetime.datetime.strptime(item['prism_coverdisplaydate'], \"%d/%B/%Y\").strftime('%Y-%m-%d') \n",
    "                    except:\n",
    "                        try:\n",
    "                            date = datetime.datetime.strptime(item['updated'], \"%Y-%m-%dT%H:%M:%SZ\").strftime('%Y-%m-%d') \n",
    "                        except:\n",
    "                            print('Error processing date!')\n",
    "                    keywords = \"\"                    \n",
    "                    language = \"en\"\n",
    "                    writers = item['author'].replace(\" and \", \",\").replace(\",\",\"#\")\n",
    "                    \n",
    "                    # data dict\n",
    "                    datadict = {\n",
    "                        'journal_title': journal,\n",
    "                        'url': url,\n",
    "                        'title': title,\n",
    "                        'writer': writers,\n",
    "                        'doi': doi,\n",
    "                        'abstract': abstract,\n",
    "                        'keyword': keywords,\n",
    "                        'publishdate': date,\n",
    "                        'language': language,\n",
    "                        'journal_eissn': feedl['eissn'],\n",
    "                        'journal_pissn': feedl['pissn'],\n",
    "                        'category': feedl['categories']\n",
    "                    }\n",
    "                    print(datadict)\n",
    "                    \n",
    "                    # send data to loader\n",
    "                    self.mur2job.put(datadict)\n",
    "                    \n",
    "                    # random sleep\n",
    "                    sleep(random.uniform(150, 450))\n",
    "                    \n",
    "            # random sleep\n",
    "            sleep(random.uniform(150, 450))\n",
    "                    \n",
    "            # check every 6-12 hour \n",
    "            print(\"Wait for next run\")\n",
    "            sleep( random.uniform(21600, 43200) )\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_mur2(threading.Thread):\n",
    "    def __init__(self, job):\n",
    "        self.filename = 'data/journal_all_'+jname+'_'+version+'.pandas'\n",
    "        threading.Thread.__init__(self)\n",
    "        self.stoprequest = threading.Event()\n",
    "        self.job = job\n",
    "        self.df =  pd.DataFrame({'url': [], 'journal_title': [],\n",
    "                                'title': [], 'writer': [], \n",
    "                                'doi': [], 'abstract': [],\n",
    "                                'keyword': [], 'publishdate': [],\n",
    "                                'language': [],\n",
    "                                'category': [],\n",
    "                                'journal_eissn': [], 'journal_pissn': []})\n",
    "        # try to read old df\n",
    "        try:\n",
    "            self.df = pd.read_pickle(self.filename)\n",
    "            print(self.df.tail())\n",
    "            print(len(self.df))\n",
    "        except:\n",
    "            pass\n",
    "    def join(self, timeout=None):\n",
    "        self.stoprequest.set()\n",
    "        super(Feed_mur2, self).join(timeout)\n",
    "    def run(self):\n",
    "        while True:\n",
    "            task = None\n",
    "            try:\n",
    "                task = self.job.get(True, 0.001) \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            # save to dataframe at the moment\n",
    "            self.df = self.df.append(dict(zip(self.df.columns,\n",
    "                                    [\n",
    "                                        task['url'], task['journal_title'],\n",
    "                                        task['title'], task['writer'], \n",
    "                                        task['doi'], task['abstract'],\n",
    "                                        task['keyword'], task['publishdate'],\n",
    "                                        task['language'], \n",
    "                                        task['category'],\n",
    "                                        task['journal_eissn'], task['journal_pissn']\n",
    "                                    ])), ignore_index=True)\n",
    "            \n",
    "            # drop duplicates\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            print(len(self.df))\n",
    "            \n",
    "            # save\n",
    "            self.df.to_pickle(self.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threadLock = threading.Lock()\n",
    "threads = []\n",
    "\n",
    "# queue for the jobs\n",
    "jobs = Queue(maxsize=0)\n",
    " \n",
    "listenerRss = ListeningRSS(jobs)\n",
    "listenerRss.start()\n",
    "threads.append(listenerRss)\n",
    " \n",
    "feedmur2 = Feed_mur2(jobs)\n",
    "feedmur2.start()\n",
    "threads.append(feedmur2)\n",
    " \n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    " \n",
    " \n",
    "print(\"Exiting Main Thread\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
