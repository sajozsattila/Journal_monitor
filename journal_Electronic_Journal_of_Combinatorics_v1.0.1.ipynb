{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"1.0.1\"\n",
    "datadir = \"data/\"\n",
    "archivedir = \"data/Archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import ui\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gedjournalarticles(filename):\n",
    "    # open old file\n",
    "    olddf = pd.DataFrame({'url': [] })\n",
    "    try:\n",
    "        olddf = pd.read_pickle(os.path.join(archivedir, filename))\n",
    "        print(olddf.tail())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    driver.get(\"https://www.combinatorics.org/ojs/index.php/eljc/issue/archive\")\n",
    "    \n",
    "    # get all of the issue\n",
    "    def issues():\n",
    "        issues = []\n",
    "        nexpage = False\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for div in soup.find_all('div', {'class': 'obj_issue_summary'}):\n",
    "            for a in div.find_all('a'):\n",
    "                # if it is this journal\n",
    "                if re.search(\"Volume\", \" \".join([ str(x) for x in a.contents ]) ):\n",
    "                    issues.append(a['href'])\n",
    "        # check there is new issue\n",
    "        for a in soup.find_all('a', {'class': 'next'}):\n",
    "            nexpage = True\n",
    "            # set to nexpage\n",
    "            driver.get(a['href'])\n",
    "            sleep(3)\n",
    "        \n",
    "        return  issues, nexpage\n",
    "    \n",
    "    nextpage = True\n",
    "    allissue = []\n",
    "    while  nextpage:\n",
    "        thispageissues, nextpage = issues()\n",
    "        allissue = allissue + thispageissues\n",
    "        print(driver.current_url, end='\\r') \n",
    "    allissue = list(set(allissue))\n",
    "    \n",
    "    # iterate over isses and get article links\n",
    "    articellinks = []\n",
    "    for issue in allissue:\n",
    "        driver.get(issue)\n",
    "        sleep(2)\n",
    "        print(driver.current_url, end='\\r') \n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # get all of the issue div\n",
    "        for div in soup.find_all('div', {'class': 'obj_article_summary'}):\n",
    "            for title in div.find_all('div', {'class': 'title'}):\n",
    "                for a in title.find_all(\"a\"):\n",
    "                    articellinks.append(a['href'])        \n",
    "    articellinks = list(set(articellinks))\n",
    "    \n",
    "    # filer out old articles\n",
    "    oldarticles = olddf['url']\n",
    "    articellinks = list( set(articellinks).difference(set(oldarticles) ))\n",
    "    print(len(articellinks), \"new article found!\")\n",
    "    \n",
    "    # iterate over article \n",
    "    urls = [ None for _ in range(len(articellinks))]\n",
    "    titles = [ None for _ in range(len(articellinks))]\n",
    "    abstracts = [ None for _ in range(len(articellinks))]\n",
    "    writers = [ None for _ in range(len(articellinks))]\n",
    "    dates = [ None for _ in range(len(articellinks))]\n",
    "    dois = [  None for _ in range(len(articellinks)) ]\n",
    "    keywords = [  None for _ in range(len(articellinks)) ]\n",
    "    for idx in range(len(articellinks)):\n",
    "        # print percentiage of the process\n",
    "        print( str(np.round(100*idx/len(articellinks),2))+\"%\" , end='\\r')\n",
    "        url =  articellinks[idx]\n",
    "        urls[idx] = url\n",
    "        # load page\n",
    "        driver.get(articellinks[idx])\n",
    "        sleep(2)\n",
    "        # get the issue article\n",
    "        # get all of the Article on this page\n",
    "        content = driver.page_source\n",
    "        # load the page content in BeautifulSoup\n",
    "        soup = BeautifulSoup(content, features=\"lxml\")\n",
    "        # process metadata\n",
    "        keyword = []\n",
    "        writter = []\n",
    "        notpaper = False\n",
    "        for m in soup.find_all(\"meta\"):\n",
    "            if m.has_attr(\"name\"):\n",
    "                if m['name'] == 'DC.Title':\n",
    "                    titles[idx] = m['content']\n",
    "                if m['name'] == 'DC.Type.articleType':\n",
    "                    if  m['content'].strip() != 'Papers' and  m['content'].strip() != 'Research Papers' and \\\n",
    "                    m['content'].strip() != 'Articles' :\n",
    "                        print(\"Not a Papers\", url)\n",
    "                        notpaper = True\n",
    "                if m['name'] == \"citation_keywords\":\n",
    "                    keyword.append( m['content'] )\n",
    "                if m['name'] == \"DC.Date.issued\":\n",
    "                    dates[idx] = datetime.datetime.strptime(m['content'], \"%Y-%m-%d\").strftime('%Y-%m-%d')\n",
    "                if m['name'] == 'DC.Creator.PersonalName':\n",
    "                    writter.append(m['content'])\n",
    "                if m['name'] == 'DC.Identifier.DOI' or m['name'] == 'citation_doi':\n",
    "                    dois[idx] = m['content']                    \n",
    "        if len(writter) > 0:\n",
    "            writers[idx] =  \"#\".join(writter)\n",
    "        if len(keyword) > 0:\n",
    "            keywords[idx] =  \"#\".join(keyword)\n",
    "            \n",
    "        # get abstract\n",
    "        abstract = \"\"\n",
    "        if notpaper:\n",
    "            continue\n",
    "        else:\n",
    "            for div in soup.find_all('div', {'class': 'abstract'}):\n",
    "                for c in div.find_all(['p', 'script']):\n",
    "                    if c.name == \"p\":                    \n",
    "                        for tag in  c.contents:\n",
    "                            if re.search(\"<span\", str(tag)):\n",
    "                                continue\n",
    "                            else:\n",
    "                                match = re.match(\"<script.+>\", str(tag))\n",
    "                                if match is not None:\n",
    "                                    abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                                else:\n",
    "                                    abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "                # if still not abstract it is possible it is not in p\n",
    "                if len(abstract) == 0:\n",
    "                    for tag in div.contents:\n",
    "                        if re.search(\"<span\", str(tag)) or re.search(\"<h3\", str(tag)) :\n",
    "                            continue\n",
    "                        else:\n",
    "                            match = re.match(\"<script.+>\", str(tag))\n",
    "                            if match is not None:\n",
    "                                abstract = abstract + \" $\"+str(tag).split('>')[1].split('<')[0] + \"$ \"\n",
    "                            else:\n",
    "                                abstract = abstract + str(tag).replace(\"\\n\", '').replace(\"\\t\", '')\n",
    "            abstracts[idx] = abstract        \n",
    "    \n",
    "    # make df\n",
    "    df = pd.DataFrame({'url': urls, \n",
    "                       'journal_title': \"Electronic Journal of Combinatorics\", \n",
    "                       'journal_eissn': \"1077-8926\",\n",
    "                       'journal_pissn': '',\n",
    "                       'category': \"Mathematics\",                                              \n",
    "                      })\n",
    "    # extend the df\n",
    "    df['title'] = titles\n",
    "    df['doi'] = dois\n",
    "    df['abstract'] = abstracts\n",
    "    df['writer'] = writers\n",
    "    df['publishdate'] = dates\n",
    "    df['keyword'] = keywords\n",
    "    \n",
    "    # drop where is no abstract\n",
    "    df = df[ df['abstract'].notna()]\n",
    "    \n",
    "    # check there was a not Arhived but previously loaded file\n",
    "    adf = None\n",
    "    try:\n",
    "        adf = pd.read_pickle(os.path.join(datadir, filename))\n",
    "    except:\n",
    "        pass\n",
    "    if adf is not None:\n",
    "        df = pd.concat([df, adf])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df, olddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4016 new article found!cs.org/ojs/index.php/eljc/issue/view/Volume10-1\n",
      "0.0%\r"
     ]
    }
   ],
   "source": [
    "filename = 'journal_Electronic_Journal_of_Combinatorics_'+version+'.pandas'\n",
    "    \n",
    "# search for articles\n",
    "df, olddf = gedjournalarticles( filename )\n",
    "\n",
    "# save data\n",
    "df.to_pickle(os.path.join(datadir, filename))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
